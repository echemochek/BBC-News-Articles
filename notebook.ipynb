{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Tagging: BBC News Articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus used in this project includes 2,225 documents from BBC's news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. \n",
    "\n",
    "The CSV file includes two columns: category (the five class labels) and text (pre-processed article content). In this project, I will use only the text column.\n",
    "\n",
    "More information on this data set as well as a paper written using this data set is available here http://mlg.ucd.ie/datasets/bbc.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  tv future in the hands of viewers with home th...\n",
       "1  worldcom boss  left books alone  former worldc...\n",
       "2  tigers wary of farrell  gamble  leicester say ...\n",
       "3  yeading face newcastle in fa cup premiership s...\n",
       "4  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/BBC-articles.csv\")\n",
    "df = df[['text']][:100]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a string and does basic text preprocessing on it.\n",
    "It returns a string\n",
    "'''\n",
    "\n",
    "def preprocess(text):\n",
    "    import contractions\n",
    "    import string\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    # load the text and convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # expand contractions\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    text = ' '.join(expanded_words)\n",
    "\n",
    "    # remove punctuations: using translate\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenize\n",
    "    tokens_raw = text.split(\" \")\n",
    "\n",
    "    # limit to tokens with more than 2 characters\n",
    "    tokens_raw = [token for token in tokens_raw if len(token) > 2]\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_filtered = [lemmatizer.lemmatize(token) for token in tokens_raw if not token in stop_words]\n",
    "    text = ' '.join(tokens_filtered)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text column using the defined function\n",
    "df['text'] = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitText = df['text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a string of text and returns a list of nouns, noun phrases and named entities.\n",
    "The function has a high complexity, and there may be more efficient ways to go about it.\n",
    "However, this gives me the output I desire more compared to available methods/packages.\n",
    "'''\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "def getNouns(text):\n",
    "        from nltk import ne_chunk, pos_tag, sent_tokenize, word_tokenize\n",
    "        from nltk.tree import Tree\n",
    "        \n",
    "        global nouns\n",
    "        nouns = []\n",
    "\n",
    "        for sentence in sent_tokenize(text):\n",
    "                for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "                        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                                nouns.append(word)\n",
    "\n",
    "                chunked = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "                continuous_chunk = []\n",
    "                current_chunk = []\n",
    "                \n",
    "                for i in chunked:\n",
    "                        if type(i) == Tree:\n",
    "                                current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "                        if current_chunk:\n",
    "                                named_entity = \" \".join(current_chunk)\n",
    "                                if named_entity not in nouns:\n",
    "                                        nouns.append(named_entity)\n",
    "                                        current_chunk = []\n",
    "                        else:\n",
    "                                continue\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dataframe, a column name (str) and a a TFIDF data format (basic, filtered & nouns)\n",
    "basic -> only basic preprocessing done before building tfidf\n",
    "filtered -> tfidf filtered to remove top 10% of the most frequent words and words that appear less than 5 times in the documents\n",
    "nouns -> tfidf built on text limited to nouns, noun phrases, and named entity recognition.\n",
    "\n",
    "It returns a dictionary and a TF-IDF corpus.\n",
    "'''\n",
    "\n",
    "def getCorpus(df=df, column=\"text\", tfidfFormat=\"basic\"):\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    # TF-IDF with basic cleaning\n",
    "    if tfidfFormat==\"basic\":\n",
    "        tokens = df[column].apply(lambda x: x.split())\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "    \n",
    "    # TF-IDF with term frequency filter cleaning    \n",
    "    elif tfidfFormat==\"filtered\":\n",
    "        tokens = df[column].apply(lambda x: x.split())\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dictionary.filter_extremes(no_below=5, no_above=0.90)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "\n",
    "    # TF-IDF with only nouns, noun phrases and NER\n",
    "    elif tfidfFormat==\"nouns\":\n",
    "        tokens = df[column].apply(lambda x: getNouns(x))\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dictionary, a corpus, the type of model (lda or lsi) and the number of topics.\n",
    "It builds a model using these parameters and returns the model and its coherence score.\n",
    "'''\n",
    "\n",
    "def buildModel(dictionary, corpus, modelType:str, num_topics):\n",
    "    from gensim.models import LsiModel,LdaModel,CoherenceModel\n",
    "\n",
    "    if modelType==\"lda\":\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        coherenceModel = CoherenceModel(model=model, texts=splitText, dictionary=dictionary, coherence='c_v')\n",
    "        coherenceScore = coherenceModel.get_coherence()\n",
    "        return model, coherenceScore\n",
    "    \n",
    "    elif modelType==\"lsi\":\n",
    "        model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        coherenceModel = CoherenceModel(model=model, texts=splitText, dictionary=dictionary, coherence='c_v')\n",
    "        coherenceScore = coherenceModel.get_coherence()\n",
    "        return model, coherenceScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a corpus and a model (like the one returned by the buildModel function).\n",
    "It returns a list of keywords found from running on the corpus\n",
    "'''\n",
    "\n",
    "def getKeywords(model, corpus):\n",
    "    n = len(corpus)\n",
    "    keywords = []\n",
    "\n",
    "    for i in range(n):    \n",
    "        for index, score in sorted(model[corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "            elements = model.print_topic(index, 5).split(\"+\")\n",
    "            keywords.append([x.strip().replace('\"', '').split(\"*\")[1] for x in elements])\n",
    "    keywords = keywords[:n]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this step we create a list of formats (basic/filtered/nouns) and model types (lda/lsa) which we will use to build our models.\n",
    "We use getKeywords() function and add new columns to the initial dataframe with these keywords.\n",
    "'''\n",
    "# dictionaries to store models and their coherence scores\n",
    "models = {'lda': [], 'lsi': []}\n",
    "coherenceScores = {'lda': [], 'lsi': []}\n",
    "\n",
    "lstFormats = ['basic', 'filtered', 'nouns']\n",
    "modelTypes = ['lda', 'lsi']\n",
    "\n",
    "# iterate through the list of TF-IDF corpus formats\n",
    "for lstFormat in lstFormats:\n",
    "    dictionary, corpus = getCorpus(df=df, column=\"text\", tfidfFormat=lstFormat)\n",
    "\n",
    "    # iterate through the list of model types\n",
    "    for modelType in modelTypes:\n",
    "        model, coherence = buildModel(dictionary=dictionary, corpus=corpus, modelType=modelType, num_topics =10)\n",
    "        \n",
    "        # save models and their coherence scores to the dictionaries\n",
    "        models[modelType].append(model)\n",
    "        coherenceScores[modelType].append(coherence)\n",
    "\n",
    "        # get keywords from the given text\n",
    "        kw = getKeywords(model, corpus)\n",
    "\n",
    "        # add keywords as new columns\n",
    "        colname = lstFormat + \"_\" + modelType\n",
    "        df[colname] = kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>basic_lda</th>\n",
       "      <th>basic_lsi</th>\n",
       "      <th>filtered_lda</th>\n",
       "      <th>filtered_lsi</th>\n",
       "      <th>nouns_lda</th>\n",
       "      <th>nouns_lsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>future hand viewer home theatre system plasma ...</td>\n",
       "      <td>[ukraine, solskjaer, tobacco, film, ntpc]</td>\n",
       "      <td>[phone, crude, oil, mobile, price]</td>\n",
       "      <td>[united, film, league, knee, oil]</td>\n",
       "      <td>[party, people, film, club, government]</td>\n",
       "      <td>[hague, growth, quarter, economy, child]</td>\n",
       "      <td>[hague, film, price, government, phone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom bos left book alone former worldcom b...</td>\n",
       "      <td>[sayeed, henman, worldcom, ebbers, tps]</td>\n",
       "      <td>[party, hague, government, price, rate]</td>\n",
       "      <td>[match, people, company, government, left]</td>\n",
       "      <td>[network, party, technology, price, phone]</td>\n",
       "      <td>[henman, blue, election, ebbers, virgin]</td>\n",
       "      <td>[party, rate, price, growth, bank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger wary farrell gamble leicester say rushed...</td>\n",
       "      <td>[fiat, human, argonaut, virgin, blue]</td>\n",
       "      <td>[hague, price, rate, party, donation]</td>\n",
       "      <td>[match, party, bbc, producer, show]</td>\n",
       "      <td>[price, share, oil, match, power]</td>\n",
       "      <td>[tobacco, music, government, broadband, sumitomo]</td>\n",
       "      <td>[hague, party, donation, share, election]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  future hand viewer home theatre system plasma ...   \n",
       "1  worldcom bos left book alone former worldcom b...   \n",
       "2  tiger wary farrell gamble leicester say rushed...   \n",
       "\n",
       "                                   basic_lda  \\\n",
       "0  [ukraine, solskjaer, tobacco, film, ntpc]   \n",
       "1    [sayeed, henman, worldcom, ebbers, tps]   \n",
       "2      [fiat, human, argonaut, virgin, blue]   \n",
       "\n",
       "                                 basic_lsi  \\\n",
       "0       [phone, crude, oil, mobile, price]   \n",
       "1  [party, hague, government, price, rate]   \n",
       "2    [hague, price, rate, party, donation]   \n",
       "\n",
       "                                 filtered_lda  \\\n",
       "0           [united, film, league, knee, oil]   \n",
       "1  [match, people, company, government, left]   \n",
       "2         [match, party, bbc, producer, show]   \n",
       "\n",
       "                                 filtered_lsi  \\\n",
       "0     [party, people, film, club, government]   \n",
       "1  [network, party, technology, price, phone]   \n",
       "2           [price, share, oil, match, power]   \n",
       "\n",
       "                                           nouns_lda  \\\n",
       "0           [hague, growth, quarter, economy, child]   \n",
       "1           [henman, blue, election, ebbers, virgin]   \n",
       "2  [tobacco, music, government, broadband, sumitomo]   \n",
       "\n",
       "                                   nouns_lsi  \n",
       "0    [hague, film, price, government, phone]  \n",
       "1         [party, rate, price, growth, bank]  \n",
       "2  [hague, party, donation, share, election]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us look at the new columns\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking through the created columns of keywords, I opine that the LSI model trained on a TF-IDF corpus that has been filtered to remove the top 10% of the most frequent words and words that appear less than 5 times in the documents does a better job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_98e7d_row0_col0 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_98e7d_row0_col1, #T_98e7d_row1_col0, #T_98e7d_row1_col1, #T_98e7d_row2_col0, #T_98e7d_row2_col1 {\n",
       "  background-color: ;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_98e7d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_98e7d_level0_col0\" class=\"col_heading level0 col0\" >lda</th>\n",
       "      <th id=\"T_98e7d_level0_col1\" class=\"col_heading level0 col1\" >lsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_98e7d_level0_row0\" class=\"row_heading level0 row0\" >basic</th>\n",
       "      <td id=\"T_98e7d_row0_col0\" class=\"data row0 col0\" >0.600662</td>\n",
       "      <td id=\"T_98e7d_row0_col1\" class=\"data row0 col1\" >0.446613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98e7d_level0_row1\" class=\"row_heading level0 row1\" >filtered</th>\n",
       "      <td id=\"T_98e7d_row1_col0\" class=\"data row1 col0\" >0.410171</td>\n",
       "      <td id=\"T_98e7d_row1_col1\" class=\"data row1 col1\" >0.380948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_98e7d_level0_row2\" class=\"row_heading level0 row2\" >nouns</th>\n",
       "      <td id=\"T_98e7d_row2_col0\" class=\"data row2 col0\" >0.594826</td>\n",
       "      <td id=\"T_98e7d_row2_col1\" class=\"data row2 col1\" >0.443263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b4837665e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for highlighting best model using coherence score\n",
    "def highlight_cells(val):\n",
    "    color = 'yellow' if val == maxVal else ''\n",
    "    return 'background-color: {}'.format(color)\n",
    "\n",
    "coherenceScoresDF = pd.DataFrame.from_dict(coherenceScores)\n",
    "coherenceScoresDF.set_index([pd.Index(lstFormats)], inplace=True)\n",
    "\n",
    "maxVal = coherenceScoresDF.max().max()\n",
    "coherenceScoresDF.style.applymap(highlight_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the coherence values, it is clear that the LDA model trained on the basically cleaned corpus performes better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Interactive for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the winning model is stored in the trainedModels list in index 4\n",
    "winningModel = models['lda'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echemochek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "C:\\Users\\echemochek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el2763218750947775206394267984\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el2763218750947775206394267984_data = {\"mdsDat\": {\"x\": [-0.01773658230327116, -0.0012346019440020615, 0.0038170290632462984, 0.0044416158373013085, 0.0032299325290394854, 0.003794976357223242, 0.0017347756277770975, 0.0015290076363373162, -0.0003833436563771546, 0.0008071908527255862], \"y\": [-0.0034214426938447818, 0.015034777239750434, -0.0051220459229520425, -0.004583883073387383, 0.0016885189892913324, -0.0011521138203029766, -0.0016639081364548318, -0.00040710727304148464, -0.00043379486190011014, 6.0999552841854384e-05], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [15.628357228547635, 13.130248907836773, 11.77605867695566, 10.590369182903196, 10.109002466884608, 10.100521980344707, 9.238991502081047, 8.25305089306266, 6.60763032585307, 4.565768835530644]}, \"tinfo\": {\"Term\": [\"hague\", \"dicaprio\", \"sayeed\", \"yukos\", \"film\", \"fox\", \"party\", \"student\", \"worldcom\", \"hingis\", \"panda\", \"henman\", \"ice\", \"pernod\", \"donation\", \"ukraine\", \"pop\", \"fiat\", \"ebbers\", \"firefox\", \"kasabian\", \"pension\", \"tps\", \"solskjaer\", \"farrell\", \"ufj\", \"tobacco\", \"pensioner\", \"hendrix\", \"winter\", \"fiat\", \"argonaut\", \"virgin\", \"blue\", \"federer\", \"human\", \"osbournes\", \"ranieri\", \"valencia\", \"vioxx\", \"roddick\", \"champion\", \"break\", \"cole\", \"drug\", \"phone\", \"merck\", \"mobile\", \"gig\", \"ozzy\", \"osbourne\", \"howard\", \"bit\", \"burger\", \"villa\", \"spain\", \"davis\", \"roger\", \"confidence\", \"award\", \"japanese\", \"right\", \"chelsea\", \"player\", \"really\", \"cup\", \"share\", \"hendrix\", \"pension\", \"bates\", \"cell\", \"broadband\", \"ferguson\", \"traffic\", \"connors\", \"ranger\", \"stem\", \"answer\", \"truant\", \"pupil\", \"tuc\", \"submission\", \"bandwidth\", \"goldsmith\", \"leeds\", \"jimmy\", \"lta\", \"connection\", \"seattle\", \"manufacturing\", \"gaming\", \"williams\", \"service\", \"halo\", \"worker\", \"lord\", \"net\", \"coach\", \"sector\", \"authority\", \"club\", \"call\", \"network\", \"people\", \"school\", \"ukraine\", \"solskjaer\", \"tobacco\", \"ntpc\", \"scholl\", \"nintendo\", \"titan\", \"benin\", \"console\", \"yushchenko\", \"sony\", \"robinson\", \"kerekou\", \"power\", \"atp\", \"ocean\", \"box\", \"sequel\", \"handheld\", \"ship\", \"psp\", \"smoking\", \"280bn\", \"india\", \"twelve\", \"law\", \"eleven\", \"viktor\", \"dvr\", \"highdefinition\", \"moya\", \"brand\", \"film\", \"league\", \"company\", \"share\", \"000\", \"united\", \"ufj\", \"sumitomo\", \"khodorkovsky\", \"blog\", \"bonus\", \"indonesia\", \"straw\", \"johansson\", \"dent\", \"rochus\", \"lebedev\", \"msn\", \"withprofits\", \"bus\", \"scott\", \"insurer\", \"paris\", \"blogging\", \"jaslyn\", \"adelaide\", \"yeading\", \"cap\", \"donation\", \"kashmir\", \"kashmiri\", \"pakistan\", \"solid\", \"dorival\", \"kronberg\", \"vivancos\", \"life\", \"region\", \"film\", \"india\", \"aid\", \"meet\", \"party\", \"debt\", \"offer\", \"bank\", \"service\", \"firefox\", \"kasabian\", \"jamieson\", \"hill\", \"dam\", \"gorge\", \"browser\", \"carpenter\", \"halloween\", \"aynsleygreen\", \"music\", \"child\", \"project\", \"kapranos\", \"construction\", \"prof\", \"meighan\", \"parliament\", \"musician\", \"opec\", \"environmental\", \"program\", \"explorer\", \"april\", \"marr\", \"easter\", \"dissolve\", \"order\", \"matter\", \"band\", \"crude\", \"government\", \"oil\", \"barrel\", \"blair\", \"dollar\", \"price\", \"election\", \"club\", \"people\", \"fox\", \"panda\", \"pernod\", \"fbi\", \"pop\", \"hutt\", \"gibbon\", \"west\", \"waiting\", \"email\", \"domecq\", \"rating\", \"recipient\", \"attachment\", \"lucas\", \"virus\", \"lewsey\", \"hiphop\", \"crude\", \"allied\", \"corp\", \"wolong\", \"reserve\", \"ball\", \"whistle\", \"disallowed\", \"nh\", \"jeffrey\", \"slowdown\", \"artist\", \"bbc\", \"rate\", \"bank\", \"show\", \"share\", \"price\", \"film\", \"try\", \"wale\", \"ice\", \"farrell\", \"singapore\", \"marsh\", \"wilkinson\", \"parker\", \"bowles\", \"hague\", \"celebrity\", \"prince\", \"insurance\", \"spitzer\", \"gamble\", \"card\", \"guilty\", \"credit\", \"connor\", \"mr\", \"25yearold\", \"korea\", \"charles\", \"replacement\", \"horgan\", \"easterby\", \"hamstring\", \"driscoll\", \"bewlay\", \"january\", \"tiger\", \"investigation\", \"injury\", \"creditor\", \"captain\", \"growth\", \"price\", \"england\", \"south\", \"rugby\", \"economy\", \"quarter\", \"student\", \"hingis\", \"journalist\", \"telegraph\", \"hague\", \"tansley\", \"motorsport\", \"disabled\", \"printing\", \"sum\", \"donation\", \"welsh\", \"newspaper\", \"colour\", \"miss\", \"gear\", \"competing\", \"body\", \"ireland\", \"tabloid\", \"reader\", \"youth\", \"party\", \"car\", \"representative\", \"display\", \"rugby\", \"rally\", \"motorcycle\", \"endeavour\", \"leadership\", \"competition\", \"referee\", \"conservative\", \"union\", \"front\", \"england\", \"wale\", \"000\", \"sayeed\", \"worldcom\", \"ebbers\", \"tps\", \"henman\", \"accounting\", \"slovakia\", \"hopman\", \"marketing\", \"intention\", \"myers\", \"voting\", \"recession\", \"raining\", \"umpire\", \"frustrating\", \"rain\", \"molik\", \"hantuchova\", \"wessels\", \"baccanello\", \"argentina\", \"mixed\", \"shaughnessy\", \"dominik\", \"vote\", \"immunisation\", \"booked\", \"match\", \"technical\", \"association\", \"party\", \"seed\", \"call\", \"japan\", \"extra\", \"growth\", \"brown\", \"dicaprio\", \"yukos\", \"insulation\", \"pensioner\", \"lion\", \"aviator\", \"discount\", \"household\", \"voucher\", \"henson\", \"hughes\", \"gazprom\", \"yuganskneftegas\", \"baikal\", \"houston\", \"auction\", \"rosneft\", \"winter\", \"scorsese\", \"\\u00a3100\", \"russian\", \"energy\", \"payment\", \"actor\", \"fuel\", \"bankruptcy\", \"russia\", \"achievement\", \"gazpromneft\", \"filing\", \"cold\", \"death\", \"oil\", \"every\", \"film\", \"court\"], \"Freq\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16681576174122598, 0.14945524912599698, 0.13672024279081124, 0.13669108696692617, 0.12227335718440642, 0.1526401016044521, 0.1119405029944864, 0.10674921634685447, 0.10491991891030547, 0.1045646518578128, 0.09962988558870295, 0.10262698187414031, 0.12255115622719792, 0.09490228623287458, 0.09979081492631937, 0.1305237399610699, 0.08570155713568256, 0.11300054929468718, 0.08814923942250054, 0.0812262812458868, 0.0811755367585143, 0.11408837629403693, 0.08507680537839832, 0.07567459268692094, 0.08361216240655799, 0.07300346455352905, 0.07628962318667752, 0.07100427959683403, 0.07893952034798735, 0.09971454717000061, 0.08540353660544187, 0.10439148152543547, 0.08516879565915675, 0.0925572057385071, 0.08436259102628682, 0.08868314551152347, 0.09193549038381942, 0.12454457528883528, 0.13488376754147985, 0.10653106310304737, 0.10562680266737014, 0.12025089088728044, 0.09549216189061875, 0.08133446839891297, 0.08034256403344069, 0.09479483034922902, 0.08218476277266748, 0.08306153597769579, 0.07441356971809722, 0.0744110053117393, 0.0742317840267721, 0.07421450099642239, 0.07064298414470803, 0.07013242018824502, 0.0784043269404816, 0.06771782457987825, 0.06769912578351844, 0.06923810350534626, 0.06489106743089196, 0.07507321650668038, 0.0653803147595048, 0.07222647168001375, 0.0979903477968554, 0.05993786312544355, 0.061927221392022044, 0.08114030143314102, 0.05982377375508249, 0.08759073839754397, 0.07200426987598521, 0.06619872100732428, 0.08435664132614237, 0.07086265493308644, 0.07125317261691949, 0.07251703094102051, 0.06868732378042654, 0.11327501685387345, 0.10905012481946513, 0.1084181003863256, 0.09266448830434817, 0.08552421856437806, 0.08316662679788084, 0.08056213439070177, 0.08054351337593137, 0.0844925340882669, 0.07267465226131542, 0.0696806338597307, 0.08346749783550575, 0.06761920546999975, 0.09075325585781346, 0.06395708254747333, 0.06782780395903906, 0.06075584920603294, 0.06780050431757124, 0.056246736639326736, 0.0562296249509385, 0.056216274599724864, 0.05551243737562456, 0.05548813242935944, 0.07436086929970961, 0.05256433963027804, 0.06325725821494466, 0.04966840348597215, 0.04831263908441516, 0.04821262824342577, 0.048193914393104095, 0.07031890522706308, 0.05280337021196719, 0.09331507385835405, 0.06680727172786277, 0.06315022181900973, 0.06254068155745206, 0.05838688957654627, 0.05620860219061555, 0.09363353787130482, 0.0760383096488943, 0.07983383720144828, 0.0741930680736921, 0.07789689857454911, 0.07105581590189408, 0.06765285557923778, 0.06693491571060312, 0.06692098124909349, 0.06689599401795966, 0.0657508150490224, 0.062274702843195494, 0.06111639197740454, 0.057002984185015745, 0.05484102043307287, 0.05176243088991862, 0.05753419041947335, 0.050312519737020146, 0.04942388136830745, 0.049413948957397605, 0.04872202511083157, 0.051529374731975476, 0.06986505233776699, 0.046354400126047204, 0.04634846437939718, 0.046343245015963534, 0.04759008037920747, 0.046159856298909584, 0.04615609124726859, 0.04614501693228452, 0.07199728887939998, 0.05056059839714744, 0.09867998901914209, 0.061220671519277965, 0.05034202610739115, 0.04732049944990876, 0.06473715434320101, 0.05477269471051789, 0.05495978384313316, 0.04998848829693639, 0.04977693440816466, 0.0891088010746751, 0.08863278948975126, 0.08322367584039848, 0.0714476720184555, 0.06768712805412139, 0.06765544090761466, 0.06594224240033582, 0.06472445440768548, 0.06470992449108189, 0.06384046888342006, 0.09798780379964356, 0.08919136349534768, 0.06654261176759914, 0.054288277538231164, 0.05231100062451415, 0.05135095155239532, 0.05126693922497038, 0.061950047553986365, 0.04999574633360752, 0.04530503063097748, 0.044613245157128154, 0.042809779671487254, 0.04277840872804887, 0.04531325705226756, 0.04140308990920682, 0.041390583178089246, 0.04138878364843204, 0.0474843073194724, 0.04338013545482505, 0.046307389216093855, 0.058392773318216816, 0.08026387629990081, 0.06159851714772135, 0.04949291147376471, 0.051607641604212336, 0.04579357722534209, 0.05909551021540856, 0.05188143232928611, 0.04870699517430364, 0.04783705626439102, 0.11238056674036985, 0.09654403789486696, 0.09458177131040883, 0.07623103510511825, 0.08899169623297483, 0.06425237398324633, 0.06424516135435054, 0.08096464646157875, 0.0673661234141957, 0.06684958822173631, 0.058697242140676356, 0.05972496983752418, 0.05542975686048543, 0.0554219226303814, 0.054350934571759095, 0.057992726523320666, 0.0614116102267221, 0.05162602495117336, 0.06928076301682136, 0.051941058608156884, 0.057657313593666513, 0.046915828952623806, 0.04894975947803361, 0.06467433845964984, 0.0450967309968669, 0.045089395075169486, 0.045958876461115857, 0.04266654768139502, 0.046761471502174, 0.04739478552658417, 0.06690202362283264, 0.07546363502172748, 0.06378599327505298, 0.056469885758297936, 0.06117771864481611, 0.058286903148027217, 0.06319735234243605, 0.0499335076050974, 0.05021752798526908, 0.08556004424007635, 0.07776666433473894, 0.07584603417426464, 0.06603015406894291, 0.0678290514353056, 0.06149534199369391, 0.06148928495780994, 0.101802552194744, 0.06040546085416392, 0.05933282041646346, 0.05270789865147186, 0.052706700401549116, 0.05175641302850853, 0.0637844551509972, 0.05331352705555908, 0.04817812862393695, 0.04801979986747909, 0.048987013108055776, 0.045181488828920605, 0.048184735445079584, 0.048406256613149165, 0.04373290335173504, 0.040440558970883334, 0.040437918122034004, 0.04042441314055185, 0.044360983670060875, 0.039398567886598804, 0.04728322389242341, 0.03878102042004481, 0.043188033267259625, 0.05681952644616404, 0.04451882372851103, 0.04698945530254165, 0.05806549021092416, 0.05809343997774945, 0.05026969500966405, 0.04639655184273198, 0.046495240646172595, 0.04615911039725755, 0.04576271992477701, 0.08059572943379043, 0.07394674825182444, 0.05775499460571309, 0.060515038949804346, 0.0928286778120463, 0.040860069465342375, 0.04085452868267043, 0.040854474114356235, 0.039961715506338334, 0.03955657515639042, 0.060406435411871504, 0.04164084129994744, 0.04427050934874267, 0.0379955309971333, 0.038109839022680676, 0.03500034339205722, 0.034802776711738544, 0.03750644778970535, 0.049118484308567176, 0.03141692405144503, 0.03141192895191501, 0.03135133923412845, 0.07211428549533176, 0.03833243576896363, 0.03414618976754987, 0.03344726200910971, 0.056083865168914636, 0.029176512775604783, 0.029175790794832377, 0.02917500794940183, 0.03313031874636761, 0.033308517872710254, 0.034866852505289536, 0.03477158882139567, 0.03722074908440162, 0.0339047144833373, 0.03566922079962731, 0.034400217862808245, 0.03428676823881876, 0.07515576646936684, 0.05623791720150265, 0.05039896438851175, 0.04981901690926099, 0.057128442808187005, 0.03873508964152182, 0.03673651107377999, 0.03673603385585978, 0.03996025591302465, 0.03639112628437778, 0.0343475380011058, 0.031929498324263755, 0.03722462771028744, 0.030392802850176516, 0.03038919683025836, 0.030388191984074272, 0.030383194638670452, 0.027686904987799, 0.027684369347511725, 0.02768368544718242, 0.02768319982753827, 0.02768240502445286, 0.027679948360370688, 0.027679615652102377, 0.027676557424585583, 0.03423350644300431, 0.03244642951322065, 0.026104932623360097, 0.04833164627612319, 0.028009191757725458, 0.027965287709066772, 0.04519815947689891, 0.0313893111634209, 0.03015916434996215, 0.029460399690649553, 0.02801335061107934, 0.028368316726470975, 0.027772064861728202, 0.04923596619733829, 0.044891206851415025, 0.027647262507171384, 0.03040923070025287, 0.028045867669432376, 0.022733500800040496, 0.02070866339359298, 0.020706615227528615, 0.02070457402801549, 0.0204473457344218, 0.02033441793883219, 0.01802828048207534, 0.018027749863089276, 0.018026210255265383, 0.018025152500568877, 0.01802499226989037, 0.018020500005432875, 0.029407285045725574, 0.01742188864036621, 0.018612912300988403, 0.019511233989106216, 0.02036869569311346, 0.018614577306734603, 0.017939252601604137, 0.01739218674915731, 0.018193404001733787, 0.01633796757740567, 0.0158255312900777, 0.014291805557614551, 0.014289208195094152, 0.016522355933424625, 0.01585458064765385, 0.02140026544569415, 0.017970974792672446, 0.022130682799117146, 0.01760434029514063], \"Total\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28276258099081286, 0.2654018932014487, 0.2526677816180142, 0.25263970035218436, 0.2382195877848561, 0.30586992275834246, 0.2278862773278388, 0.22269465146033462, 0.2208662052027128, 0.22050990175911986, 0.21557536856904494, 0.22265612908304105, 0.2700299665272589, 0.21084841036199103, 0.22586969436838028, 0.30222668181260925, 0.2016468713175136, 0.26872635054313065, 0.2104469771666935, 0.19717139254960075, 0.19712149110173005, 0.28211157072669996, 0.21132279506207696, 0.19161956031404875, 0.21413667390507612, 0.1889485254925999, 0.1987476937165752, 0.1869497169031236, 0.20820838465616703, 0.2630669133617112, 0.2324101236030639, 0.296865228959083, 0.23183815725374785, 0.29438422949070464, 0.24581403158488796, 0.28083744673985506, 0.32224697602141983, 0.2430714602331396, 0.2737117912201342, 0.22505668182656763, 0.22415322818912456, 0.25586766050287213, 0.2140180566454677, 0.19986008760974075, 0.19886791447270857, 0.2354158211201598, 0.2078834563290758, 0.21206741298856122, 0.19293874497796518, 0.19293608485040356, 0.1927570475505574, 0.19274011734971547, 0.18916832279483597, 0.1886575969972591, 0.21336109967981967, 0.18624284007408023, 0.1862245039237767, 0.19392395501793605, 0.18341648583524595, 0.21417164064563649, 0.18783022013445508, 0.20850999998221292, 0.28479872730754185, 0.1784630635326767, 0.18540819848140194, 0.24580747511254375, 0.18464818660654958, 0.2728650706472898, 0.22756665478609664, 0.2087376646071724, 0.32639826478037465, 0.2501390482855227, 0.2611616959674493, 0.34950552485172215, 0.23499597701335484, 0.23315423268050306, 0.22893055734961698, 0.22829736132884298, 0.21254439445942838, 0.20799503430068383, 0.20304619040544714, 0.20044108320812967, 0.20042281837955797, 0.2109777727254915, 0.1925529555828121, 0.18956020740353538, 0.22971502788115442, 0.18749768785055315, 0.2599858562223051, 0.1838351517046356, 0.19572768314331587, 0.1806339206445616, 0.20207410026458844, 0.176124750155212, 0.17610810166163482, 0.1760951603117051, 0.1753904364832478, 0.17536665052108272, 0.24107171443001557, 0.17244184668330817, 0.21442375966646363, 0.16954654401797772, 0.16819033254286267, 0.16808999393502932, 0.16807149126883703, 0.2467380073910192, 0.18668939618400987, 0.3915084013894716, 0.28087843475635216, 0.2979201330690849, 0.32224697602141983, 0.28236041161256753, 0.2436958915324899, 0.2149553398506327, 0.197358939244263, 0.20750029122897595, 0.19551515577146486, 0.20667745254191414, 0.19237576108559745, 0.1889729852716434, 0.18825517916186485, 0.18824172561770028, 0.1882175013373151, 0.18707093194155494, 0.18359487675424188, 0.18243633730527561, 0.17832264295534167, 0.18111378909513384, 0.17308232872719062, 0.19562761753009442, 0.17163212091990673, 0.17074362175692184, 0.17073391296610366, 0.17004134012328262, 0.18459617155841368, 0.2511459931593845, 0.16767361245703002, 0.1676678977254932, 0.16766289034579768, 0.17255712655774808, 0.16747922057906114, 0.16747553408502758, 0.16746473297470935, 0.26487822983062953, 0.185106683010267, 0.3915084013894716, 0.24107171443001557, 0.1869925611311757, 0.17482294809347917, 0.36037845137390717, 0.24617270294216523, 0.26137668050814955, 0.27935341216697324, 0.28479872730754185, 0.21093862112707845, 0.21046295010990373, 0.20505406549955107, 0.19823737070872402, 0.1895171064406847, 0.1894858207819599, 0.18777183486731214, 0.18655365588518985, 0.18653956979568675, 0.1856703115236408, 0.29907624662677834, 0.28648893385950525, 0.21687071603425653, 0.1770731697653447, 0.17414050303840473, 0.1731803074479412, 0.1730957914061323, 0.21147700689856572, 0.17249049030768687, 0.16713448760347696, 0.16644266588100973, 0.16463870848518491, 0.16460792361583412, 0.17657013460902055, 0.16323209188193608, 0.16321983634183737, 0.1632181185406742, 0.18790553569434254, 0.17201270853541648, 0.18569837453479762, 0.23567536320244512, 0.32934223066338564, 0.2566217629633141, 0.20460704918748077, 0.22057221803131433, 0.1848641950002227, 0.311917101394265, 0.262589549359653, 0.32639826478037465, 0.34950552485172215, 0.23426360527686585, 0.21842517599212793, 0.2164637590053049, 0.19811236424273493, 0.23430759447012467, 0.1861324691583135, 0.18612543011462712, 0.2388408776350024, 0.20055007197501007, 0.19976888524380357, 0.1805775928492459, 0.18962372003437822, 0.17731023517044076, 0.17730276721121643, 0.17623118336585708, 0.19058316231631733, 0.20246616662624106, 0.17350580109178781, 0.23567536320244512, 0.1777261181740587, 0.20562910579340682, 0.168795633354176, 0.17876702544361525, 0.23677350254080975, 0.1669765565725101, 0.16696946571939192, 0.17530151496175933, 0.1645462885529709, 0.1845262860066127, 0.18797219463265766, 0.26601634657548984, 0.3024122972978331, 0.27935341216697324, 0.26150976693411965, 0.32224697602141983, 0.311917101394265, 0.3915084013894716, 0.22075744488174942, 0.24557821270979596, 0.21610540535929632, 0.20069045980792097, 0.19876986730546609, 0.18895359248705973, 0.19483656258691753, 0.18441796378007216, 0.18441226644673023, 0.3059057125917077, 0.18332852625444332, 0.1886821465114975, 0.17563131700819248, 0.17563001925860142, 0.1746798240549889, 0.21856641588953102, 0.19497683496925464, 0.17690396485472423, 0.17674631860760973, 0.180702371788167, 0.1681043001346013, 0.180188493773213, 0.18559866849334158, 0.17355105663908105, 0.16336293669967714, 0.16336029331688107, 0.16334720842785352, 0.18257381466501008, 0.162321421528447, 0.19487865648272099, 0.16170344004615098, 0.18119989338345757, 0.24463955439318177, 0.18861785381770824, 0.20339230736189487, 0.2800677454839532, 0.311917101394265, 0.3049594938130628, 0.25120886125505415, 0.29112904464996575, 0.27361141169043857, 0.26667715106197043, 0.20465310217008806, 0.1980036111642958, 0.1818111352701269, 0.19696765634181965, 0.3059057125917077, 0.16491529020567772, 0.16490991045881376, 0.1649098559654736, 0.1640172254463356, 0.16361193734793725, 0.2511459931593845, 0.17715520416901082, 0.18850853133479725, 0.16205102749194675, 0.16733547027803866, 0.1590558790930766, 0.15885825379611515, 0.17587452165100184, 0.23520300027787472, 0.1554721280287332, 0.15546722577012273, 0.1554068251882371, 0.36037845137390717, 0.19435698085352582, 0.1733980722481323, 0.17198705894087693, 0.29112904464996575, 0.1532315228318222, 0.15323083519380618, 0.15323002984377024, 0.1749252309869368, 0.17591071134761083, 0.1854739107469858, 0.19809447457542148, 0.25584923724446895, 0.19469602785275836, 0.3049594938130628, 0.24557821270979596, 0.28236041161256753, 0.20129629878442795, 0.18237721442438087, 0.1765383525527762, 0.1759579118603904, 0.20589805019939228, 0.16487360287659286, 0.16287504156021834, 0.1628746380628226, 0.17733308706685072, 0.1625297487048731, 0.16768204464086034, 0.15806763100713278, 0.1848159888514666, 0.15653068714191581, 0.1565273655394911, 0.15652642881737813, 0.15652154190249035, 0.15382479767103582, 0.15382234363853886, 0.1538217647047264, 0.15382120627539198, 0.15382039894241875, 0.15381802670796826, 0.15381786132539388, 0.15381480169169934, 0.19091394457996072, 0.18656428710889375, 0.15224291015055835, 0.28224975668802665, 0.1637350803707548, 0.16733340955651857, 0.36037845137390717, 0.20485087310905006, 0.2501390482855227, 0.2678427073809954, 0.17208339323402078, 0.2800677454839532, 0.19155131523588026, 0.17812737274939838, 0.20648541717230384, 0.1565366166993506, 0.17544912307802163, 0.16434386781193605, 0.1516221922120905, 0.14959724417496525, 0.14959537984035837, 0.14959345094281212, 0.15559031674322188, 0.16030745439832067, 0.1469166879051769, 0.14691618508125015, 0.14691457944950706, 0.1469136563284458, 0.1469134959756037, 0.1469092475488099, 0.24452468896005938, 0.14631083960764402, 0.15810737097679475, 0.17021194420420993, 0.18609099437202475, 0.17156455509833884, 0.16723488257767777, 0.16415404313306492, 0.17272703731604205, 0.15901913642149026, 0.15575136261744615, 0.14317998069315088, 0.14317751636405057, 0.168109540339167, 0.16529814134742685, 0.2566217629633141, 0.20602868425928708, 0.3915084013894716, 0.24054456759269338], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.7076, -6.8175, -6.9066, -6.9068, -7.0182, -6.7964, -7.1065, -7.154, -7.1713, -7.1747, -7.223, -7.1934, -7.016, -7.2717, -7.2214, -6.9529, -7.3736, -7.0971, -7.3455, -7.4273, -7.4279, -7.0875, -7.3809, -7.4981, -7.3983, -7.534, -7.49, -7.5618, -7.4558, -7.2222, -7.3771, -7.1764, -7.3799, -7.2967, -7.3894, -7.3394, -7.3034, -6.8257, -6.7459, -6.9819, -6.9904, -6.8608, -7.0913, -7.2518, -7.264, -7.0986, -7.2414, -7.2308, -7.3407, -7.3407, -7.3431, -7.3434, -7.3927, -7.3999, -7.2885, -7.435, -7.4353, -7.4128, -7.4776, -7.3319, -7.4701, -7.3705, -7.0655, -7.557, -7.5244, -7.2542, -7.5589, -7.1777, -7.3736, -7.4577, -7.2153, -7.3896, -7.3841, -7.3665, -7.4208, -6.8117, -6.8497, -6.8555, -7.0125, -7.0927, -7.1206, -7.1525, -7.1527, -7.1048, -7.2555, -7.2976, -7.117, -7.3276, -7.0333, -7.3833, -7.3245, -7.4346, -7.3249, -7.5117, -7.512, -7.5123, -7.5249, -7.5253, -7.2326, -7.5794, -7.3943, -7.6361, -7.6638, -7.6659, -7.6663, -7.2884, -7.5749, -7.0055, -7.3397, -7.396, -7.4057, -7.4744, -7.5124, -6.896, -7.1041, -7.0554, -7.1287, -7.08, -7.1719, -7.221, -7.2316, -7.2318, -7.2322, -7.2495, -7.3038, -7.3226, -7.3923, -7.4309, -7.4887, -7.383, -7.5171, -7.5349, -7.5351, -7.5492, -7.4932, -7.1888, -7.599, -7.5992, -7.5993, -7.5727, -7.6032, -7.6033, -7.6036, -7.1587, -7.5122, -6.8435, -7.3209, -7.5165, -7.5784, -7.265, -7.4322, -7.4288, -7.5236, -7.5278, -6.899, -6.9043, -6.9673, -7.1199, -7.1739, -7.1744, -7.2001, -7.2187, -7.2189, -7.2325, -6.804, -6.8981, -7.191, -7.3945, -7.4316, -7.4502, -7.4518, -7.2625, -7.4769, -7.5754, -7.5908, -7.6321, -7.6328, -7.5752, -7.6655, -7.6658, -7.6658, -7.5284, -7.6188, -7.5535, -7.3216, -7.0035, -7.2682, -7.487, -7.4452, -7.5647, -7.3097, -7.4399, -7.503, -7.521, -6.6661, -6.818, -6.8385, -7.0542, -6.8995, -7.2252, -7.2253, -6.994, -7.1779, -7.1856, -7.3156, -7.2983, -7.3729, -7.373, -7.3925, -7.3277, -7.2704, -7.444, -7.1498, -7.4379, -7.3335, -7.5396, -7.4972, -7.2186, -7.5792, -7.5794, -7.5603, -7.6346, -7.5429, -7.5295, -7.1848, -7.0644, -7.2325, -7.3543, -7.2742, -7.3226, -7.2417, -7.4773, -7.4716, -6.8496, -6.9451, -6.9701, -7.1087, -7.0819, -7.1799, -7.18, -6.6758, -7.1978, -7.2157, -7.3341, -7.3341, -7.3523, -7.1433, -7.3227, -7.4239, -7.4272, -7.4073, -7.4882, -7.4238, -7.4192, -7.5207, -7.599, -7.5991, -7.5994, -7.5065, -7.6251, -7.4427, -7.6409, -7.5333, -7.259, -7.5029, -7.4489, -7.2373, -7.2368, -7.3814, -7.4616, -7.4595, -7.4668, -7.4754, -6.7966, -6.8827, -7.1298, -7.0831, -6.6552, -7.4758, -7.476, -7.476, -7.4981, -7.5083, -7.0849, -7.4569, -7.3957, -7.5485, -7.5455, -7.6306, -7.6363, -7.5615, -7.2918, -7.7387, -7.7388, -7.7407, -6.9077, -7.5397, -7.6553, -7.676, -7.1591, -7.8126, -7.8127, -7.8127, -7.6855, -7.6802, -7.6345, -7.6372, -7.5691, -7.6624, -7.6117, -7.6479, -7.6512, -6.6441, -6.934, -7.0437, -7.0552, -6.9183, -7.3069, -7.3599, -7.3599, -7.2758, -7.3693, -7.4271, -7.5001, -7.3467, -7.5494, -7.5496, -7.5496, -7.5498, -7.6427, -7.6428, -7.6428, -7.6428, -7.6428, -7.6429, -7.6429, -7.6431, -7.4304, -7.484, -7.7015, -7.0856, -7.6311, -7.6327, -7.1526, -7.5172, -7.5572, -7.5806, -7.631, -7.6184, -7.6396, -6.6974, -6.7898, -7.2745, -7.1793, -7.2602, -7.4702, -7.5634, -7.5635, -7.5636, -7.5761, -7.5817, -7.7021, -7.7021, -7.7022, -7.7022, -7.7022, -7.7025, -7.2128, -7.7363, -7.6701, -7.623, -7.58, -7.6701, -7.707, -7.738, -7.6929, -7.8005, -7.8324, -7.9343, -7.9345, -7.7893, -7.8305, -7.5306, -7.7052, -7.497, -7.7259], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3284, 1.2818, 1.2419, 1.2418, 1.1891, 1.161, 1.1452, 1.1208, 1.1117, 1.1099, 1.0842, 1.0816, 1.0661, 1.0578, 1.0392, 1.0165, 1.0004, 0.9898, 0.9859, 0.9692, 0.9689, 0.9508, 0.9463, 0.927, 0.9157, 0.9051, 0.8986, 0.888, 0.8862, 0.886, 0.855, 0.811, 0.8547, 0.699, 0.7866, 0.7034, 0.6019, 1.3616, 1.3226, 1.2823, 1.2778, 1.2752, 1.2232, 1.1312, 1.1239, 1.1206, 1.1022, 1.0929, 1.0775, 1.0775, 1.076, 1.0759, 1.0453, 1.0407, 1.0291, 1.0185, 1.0184, 1.0003, 0.9912, 0.9819, 0.9749, 0.9701, 0.9633, 0.9392, 0.9337, 0.9219, 0.9032, 0.8939, 0.8795, 0.8818, 0.6772, 0.769, 0.7314, 0.4576, 0.8002, 1.4172, 1.3975, 1.3944, 1.3089, 1.2504, 1.2465, 1.2276, 1.2275, 1.224, 1.1647, 1.1383, 1.1267, 1.1192, 1.0866, 1.0833, 1.0793, 1.0495, 1.047, 0.9977, 0.9974, 0.9973, 0.9887, 0.9884, 0.9629, 0.9511, 0.9184, 0.9113, 0.8917, 0.8902, 0.8899, 0.8838, 0.8762, 0.7051, 0.703, 0.5878, 0.4996, 0.563, 0.6723, 1.4142, 1.2914, 1.29, 1.2763, 1.2695, 1.2492, 1.218, 1.2111, 1.211, 1.2108, 1.1996, 1.164, 1.1516, 1.1047, 1.0505, 1.0381, 1.0214, 1.0181, 1.0055, 1.0054, 0.9953, 0.9692, 0.9658, 0.9595, 0.9594, 0.9593, 0.9571, 0.9565, 0.9564, 0.9562, 0.9426, 0.9475, 0.8671, 0.8746, 0.933, 0.9384, 0.5284, 0.7424, 0.6859, 0.5245, 0.501, 1.43, 1.4269, 1.39, 1.2712, 1.2622, 1.2619, 1.2453, 1.2332, 1.233, 1.2242, 1.1759, 1.1248, 1.1103, 1.1095, 1.0891, 1.0761, 1.0749, 1.064, 1.0533, 0.9864, 0.9751, 0.9448, 0.9442, 0.9316, 0.9199, 0.9197, 0.9197, 0.9162, 0.9142, 0.9029, 0.8965, 0.88, 0.8648, 0.8725, 0.8392, 0.8963, 0.6282, 0.6701, 0.3894, 0.303, 1.558, 1.4761, 1.4646, 1.3375, 1.3245, 1.2289, 1.2289, 1.2108, 1.2017, 1.1979, 1.1688, 1.1373, 1.1298, 1.1297, 1.1162, 1.1028, 1.0996, 1.0804, 1.0683, 1.0624, 1.021, 1.0122, 0.9973, 0.9948, 0.9835, 0.9834, 0.9538, 0.9428, 0.9199, 0.9148, 0.9123, 0.9044, 0.8156, 0.7598, 0.631, 0.6152, 0.4688, 0.8062, 0.7053, 1.4552, 1.4337, 1.4183, 1.3303, 1.3266, 1.2835, 1.2834, 1.2815, 1.2715, 1.2248, 1.1781, 1.1781, 1.1653, 1.1502, 1.085, 1.081, 1.0786, 1.0764, 1.0678, 1.0628, 1.0378, 1.0034, 0.9856, 0.9855, 0.9853, 0.9669, 0.9659, 0.9655, 0.9539, 0.9477, 0.9218, 0.9379, 0.9165, 0.8083, 0.7011, 0.579, 0.6927, 0.5473, 0.6021, 0.6192, 1.5627, 1.5096, 1.3478, 1.3144, 1.3021, 1.0993, 1.0992, 1.0992, 1.0825, 1.0748, 1.0696, 1.0466, 1.0458, 1.0441, 1.0151, 0.9807, 0.9763, 0.9493, 0.9284, 0.8955, 0.8953, 0.8938, 0.8857, 0.8712, 0.8696, 0.8571, 0.8477, 0.836, 0.836, 0.836, 0.8307, 0.8304, 0.8232, 0.7546, 0.5669, 0.7467, 0.3487, 0.529, 0.3862, 1.7317, 1.5405, 1.4634, 1.4551, 1.4349, 1.2685, 1.2277, 1.2277, 1.2268, 1.2204, 1.1314, 1.1175, 1.1146, 1.0779, 1.0778, 1.0778, 1.0776, 1.0021, 1.002, 1.002, 1.002, 1.002, 1.0019, 1.0019, 1.0018, 0.9983, 0.9678, 0.9536, 0.9522, 0.9512, 0.9279, 0.6408, 0.8411, 0.6014, 0.5096, 0.9016, 0.4272, 0.7858, 1.8007, 1.5606, 1.3528, 1.334, 1.3185, 1.189, 1.1092, 1.1091, 1.109, 1.0572, 1.0218, 0.9887, 0.9886, 0.9886, 0.9885, 0.9885, 0.9883, 0.9685, 0.9586, 0.9472, 0.9205, 0.8743, 0.8656, 0.8542, 0.8418, 0.8359, 0.8111, 0.7999, 0.7822, 0.782, 0.7667, 0.7423, 0.6024, 0.6473, 0.2135, 0.4718]}, \"token.table\": {\"Topic\": [], \"Freq\": [], \"Term\": []}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 10, 1, 6, 7, 9, 8, 4, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el2763218750947775206394267984\", ldavis_el2763218750947775206394267984_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el2763218750947775206394267984\", ldavis_el2763218750947775206394267984_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el2763218750947775206394267984\", ldavis_el2763218750947775206394267984_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "4     -0.017737 -0.003421       1        1  15.628357\n",
       "9     -0.001235  0.015035       2        1  13.130249\n",
       "0      0.003817 -0.005122       3        1  11.776059\n",
       "5      0.004442 -0.004584       4        1  10.590369\n",
       "6      0.003230  0.001689       5        1  10.109002\n",
       "8      0.003795 -0.001152       6        1  10.100522\n",
       "7      0.001735 -0.001664       7        1   9.238992\n",
       "3      0.001529 -0.000407       8        1   8.253051\n",
       "2     -0.000383 -0.000434       9        1   6.607630\n",
       "1      0.000807  0.000061      10        1   4.565769, topic_info=          Term      Freq     Total Category  logprob  loglift\n",
       "1347     hague  0.000000  0.000000  Default  30.0000  30.0000\n",
       "2223  dicaprio  0.000000  0.000000  Default  29.0000  29.0000\n",
       "2820    sayeed  0.000000  0.000000  Default  28.0000  28.0000\n",
       "4696     yukos  0.000000  0.000000  Default  27.0000  27.0000\n",
       "629       film  0.000000  0.000000  Default  26.0000  26.0000\n",
       "...        ...       ...       ...      ...      ...      ...\n",
       "2316     death  0.015855  0.165298  Topic10  -7.8305   0.7423\n",
       "1291       oil  0.021400  0.256622  Topic10  -7.5306   0.6024\n",
       "1633     every  0.017971  0.206029  Topic10  -7.7052   0.6473\n",
       "629       film  0.022131  0.391508  Topic10  -7.4970   0.2135\n",
       "1395     court  0.017604  0.240545  Topic10  -7.7259   0.4718\n",
       "\n",
       "[416 rows x 6 columns], token_table=Empty DataFrame\n",
       "Columns: [Topic, Freq, Term]\n",
       "Index: [], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 10, 1, 6, 7, 9, 8, 4, 3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interacting with LDA output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "d, c = getCorpus(df=df, column=\"text\", tfidfFormat=\"basic\")\n",
    "\n",
    "vis = gensimvis.prepare(winningModel, c, d)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidently, there is an overlap among many of the topics as seen in the visualization above. It is important to objectively select the best number of topics on which to base the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
