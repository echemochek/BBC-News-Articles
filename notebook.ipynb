{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Tagging: BBC News Articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus used in this project includes 2,225 documents from BBC's news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. \n",
    "\n",
    "The CSV file includes two columns: category (the five class labels) and text (pre-processed article content). In this project, I will use only the text column.\n",
    "\n",
    "More information on this data set as well as a paper written using this data set is available here http://mlg.ucd.ie/datasets/bbc.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  tv future in the hands of viewers with home th...\n",
       "1  worldcom boss  left books alone  former worldc...\n",
       "2  tigers wary of farrell  gamble  leicester say ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/BBC-articles.csv\")\n",
    "df = df[['text']][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a df and name of column (containing sentences) in the df.\n",
    "The input is split to tokens which are lemmatized, and stopwords removed.\n",
    "The output is a list of lists.  \n",
    "'''\n",
    "import re\n",
    "def preprocess_text(text):    \n",
    "    cleanTokens=[]\n",
    "    lem = WordNetLemmatizer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    for txt in text:\n",
    "        words = [lem.lemmatize(w) for w in word_tokenize(txt) if (w not in stop) and len(w)>2]\n",
    "        cleanTokens.append(words)        \n",
    "\n",
    "    return cleanTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a string of text and returns a list of nouns, noun phrases and named entities.\n",
    "The function has a high complexity, and there may be more efficient ways to go about it.\n",
    "However, this gives me the output I desire more compared to available methods/packages.\n",
    "'''\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "def getNouns(text):\n",
    "        from nltk import ne_chunk, pos_tag, sent_tokenize, word_tokenize\n",
    "        from nltk.tree import Tree\n",
    "        \n",
    "        global nouns\n",
    "        nouns = []\n",
    "\n",
    "        for sentence in sent_tokenize(text):\n",
    "                for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "                        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                                nouns.append(word)\n",
    "\n",
    "                chunked = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "                continuous_chunk = []\n",
    "                current_chunk = []\n",
    "                \n",
    "                for i in chunked:\n",
    "                        if type(i) == Tree:\n",
    "                                current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "                        if current_chunk:\n",
    "                                named_entity = \" \".join(current_chunk)\n",
    "                                if named_entity not in nouns:\n",
    "                                        nouns.append(named_entity)\n",
    "                                        current_chunk = []\n",
    "                        else:\n",
    "                                continue\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dataframe, a column name (str) and a a TFIDF data format (basic, filtered & nouns)\n",
    "basic -> only basic preprocessing done before building tfidf\n",
    "filtered -> tfidf filtered to remove top 10% of the most frequent words and words that appear less than 5 times in the documents\n",
    "nouns -> tfidf built on text limited to nouns, noun phrases, and named entity recognition.\n",
    "\n",
    "It returns a dictionary and a TF-IDF corpus.\n",
    "'''\n",
    "\n",
    "def getCorpus(df=df, column=\"text\", tfidfFormat=\"basic\"):\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    # TF-IDF with basic cleaning\n",
    "    if tfidfFormat==\"basic\":\n",
    "        tokens = preprocess_text(df[column])\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "    \n",
    "    # TF-IDF with term frequency filter cleaning    \n",
    "    elif tfidfFormat==\"filtered\":\n",
    "        tokens = preprocess_text(df[column])\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dictionary.filter_extremes(no_below=5, no_above=0.90)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "\n",
    "    # TF-IDF with only nouns, noun phrases and NER\n",
    "    elif tfidfFormat==\"nouns\":\n",
    "        tokens = df[column].apply(lambda x: getNouns(x))\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dictionary, a corpus, the type of model (lda or lsi) and the number of topics.\n",
    "It builds a model using these parameters and return the model.\n",
    "'''\n",
    "\n",
    "def buildModel(dictionary, corpus, modelType:str, num_topics):\n",
    "    from gensim.models import LsiModel,LdaModel\n",
    "\n",
    "    if modelType==\"lda\":\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        return model\n",
    "    \n",
    "    elif modelType==\"lsi\":\n",
    "        model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a corpus and a model (like the one returned by the buildModel function).\n",
    "It returns a list of keywords found from running on the corpus\n",
    "'''\n",
    "\n",
    "def getKeywords(model, corpus):\n",
    "    n = len(corpus)\n",
    "    keywords = []\n",
    "\n",
    "    for i in range(n):    \n",
    "        for index, score in sorted(model[corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "            elements = model.print_topic(index, 5).split(\"+\")\n",
    "            keywords.append([x.strip().replace('\"', '').split(\"*\")[1] for x in elements])\n",
    "    keywords = keywords[:n]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this step we create a list of formats (basic/filtered/nouns) and model types (lda/lsa) which we will use to build our models.\n",
    "We use getKeywords() function and add new columns to the initial dataframe with these keywords.\n",
    "'''\n",
    "\n",
    "lstFormats = ['basic', 'filtered', 'nouns']\n",
    "modelTypes = ['lda', 'lsi']\n",
    "\n",
    "# iterate through the list of TF-IDF corpus formats\n",
    "for lstFormat in lstFormats:\n",
    "    dictionary, corpus = getCorpus(df=df, column=\"text\", tfidfFormat=lstFormat)\n",
    "\n",
    "    # iterate through the list of model types\n",
    "    for modelType in modelTypes:\n",
    "        model = buildModel(dictionary=dictionary, corpus=corpus, modelType=modelType, num_topics =10)\n",
    "\n",
    "        # get keywords from the given text\n",
    "        kw = getKeywords(model, corpus)\n",
    "\n",
    "        # add keywords as new columns\n",
    "        colname = lstFormat + \"_\" + modelType\n",
    "        df[colname] = kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us look at the new columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking through the created columns of keywords, I opine that the LSI model trained on a TF-IDF corpus that has been filtered to remove the top 10% of the most frequent words and words that appear less than 5 times in the documents does a better job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get best model using coherence scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining optimum number of topics using coherence values for tfidfLtd\n",
    "\n",
    "#def getOptimalTopics(min_topics=1,max_topics=10,step=1):\n",
    "    \n",
    "\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "min_topics, max_topics, step = 1, 10, 1\n",
    "\n",
    "for i in range(min_topics, max_topics, step):\n",
    "    model = LsiModel(tfidfLtd, id2word=dictionaryLtd, num_topics=i)\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(model=model, texts=tokens, dictionary=dictionaryLtd, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = range(min_topics, max_topics, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.legend((\"coherence_values\"), loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Interactive for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interacting with LDA output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = gensimvis.prepare(lda_Ltd, tfidfLtd, dictionaryLtd)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating LDA models: Topic coherence\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "goodLdaModel = LdaModel(corpus=tfidfNouns, id2word=dictionaryNouns, iterations=50, num_topics=2)\n",
    "badLdaModel = LdaModel(corpus=tfidfNouns, id2word=dictionaryNouns, iterations=1, num_topics=2)\n",
    "\n",
    "goodcm = CoherenceModel(model=goodLdaModel, corpus=tfidfNouns, dictionary=dictionaryNouns, coherence='u_mass')\n",
    "badcm  = CoherenceModel(model=badLdaModel, corpus=tfidfNouns, dictionary=dictionaryNouns, coherence='u_mass')\n",
    "goodcm = CoherenceModel(model=goodLdaModel, texts=tfidfNouns, dictionary=dictionaryNouns, coherence='c_v')\n",
    "badcm  = CoherenceModel(model=badLdaModel, texts=tfidfNouns, dictionary=dictionaryNouns, coherence='c_v')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
