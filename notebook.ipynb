{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Tagging: BBC News Articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus used in this project includes 2,225 documents from BBC's news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. \n",
    "\n",
    "The CSV file includes two columns: category (the five class labels) and text (pre-processed article content). In this project, I will use only the text column.\n",
    "\n",
    "More information on this data set as well as a paper written using this data set is available here http://mlg.ucd.ie/datasets/bbc.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  tv future in the hands of viewers with home th...\n",
       "1  worldcom boss  left books alone  former worldc...\n",
       "2  tigers wary of farrell  gamble  leicester say ...\n",
       "3  yeading face newcastle in fa cup premiership s...\n",
       "4  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/BBC-articles.csv\")\n",
    "df = df[['text']]#[:100]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a string and does basic text preprocessing on it.\n",
    "It returns a string\n",
    "'''\n",
    "\n",
    "def preprocess(text):\n",
    "    import contractions\n",
    "    import string\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    # load the text and convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # expand contractions\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    text = ' '.join(expanded_words)\n",
    "\n",
    "    # remove punctuations: using translate\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenize\n",
    "    tokens_raw = text.split(\" \")\n",
    "\n",
    "    # limit to tokens with more than 2 characters\n",
    "    tokens_raw = [token for token in tokens_raw if len(token) > 2]\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_filtered = [lemmatizer.lemmatize(token) for token in tokens_raw if not token in stop_words]\n",
    "    text = ' '.join(tokens_filtered)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text column using the defined function\n",
    "df['text'] = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitText = df['text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a string of text and returns a list of nouns, noun phrases and named entities.\n",
    "The function has a high complexity, and there may be more efficient ways to go about it.\n",
    "However, this gives me the output I desire more compared to available methods/packages.\n",
    "'''\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "def getNouns(text):\n",
    "        from nltk import ne_chunk, pos_tag, sent_tokenize, word_tokenize\n",
    "        from nltk.tree import Tree\n",
    "        \n",
    "        global nouns\n",
    "        nouns = []\n",
    "\n",
    "        for sentence in sent_tokenize(text):\n",
    "                for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "                        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                                nouns.append(word)\n",
    "\n",
    "                chunked = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "                continuous_chunk = []\n",
    "                current_chunk = []\n",
    "                \n",
    "                for i in chunked:\n",
    "                        if type(i) == Tree:\n",
    "                                current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "                        if current_chunk:\n",
    "                                named_entity = \" \".join(current_chunk)\n",
    "                                if named_entity not in nouns:\n",
    "                                        nouns.append(named_entity)\n",
    "                                        current_chunk = []\n",
    "                        else:\n",
    "                                continue\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dataframe, a column name (str) and a a TFIDF data format (basic, filtered & nouns)\n",
    "basic -> only basic preprocessing done before building tfidf\n",
    "filtered -> tfidf filtered to remove top 10% of the most frequent words and words that appear less than 5 times in the documents\n",
    "nouns -> tfidf built on text limited to nouns, noun phrases, and named entity recognition.\n",
    "\n",
    "It returns a dictionary and a TF-IDF corpus.\n",
    "'''\n",
    "\n",
    "def getCorpus(df=df, column=\"text\", tfidfFormat=\"basic\"):\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    # TF-IDF with basic cleaning\n",
    "    if tfidfFormat==\"basic\":\n",
    "        tokens = df[column].apply(lambda x: x.split())\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "    \n",
    "    # TF-IDF with term frequency filter cleaning    \n",
    "    elif tfidfFormat==\"filtered\":\n",
    "        tokens = df[column].apply(lambda x: x.split())\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dictionary.filter_extremes(no_below=5, no_above=0.90)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "\n",
    "    # TF-IDF with only nouns, noun phrases and NER\n",
    "    elif tfidfFormat==\"nouns\":\n",
    "        tokens = df[column].apply(lambda x: getNouns(x))\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dictionary, a corpus, the type of model (lda or lsi) and the number of topics.\n",
    "It builds a model using these parameters and returns the model and its coherence score.\n",
    "'''\n",
    "\n",
    "def buildModel(dictionary, corpus, modelType:str, num_topics):\n",
    "    from gensim.models import LsiModel,LdaModel,CoherenceModel\n",
    "\n",
    "    if modelType==\"lda\":\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        coherenceModel = CoherenceModel(model=model, texts=splitText, dictionary=dictionary, coherence='c_v')\n",
    "        coherenceScore = coherenceModel.get_coherence()\n",
    "        return model, coherenceScore\n",
    "    \n",
    "    elif modelType==\"lsi\":\n",
    "        model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        coherenceModel = CoherenceModel(model=model, texts=splitText, dictionary=dictionary, coherence='c_v')\n",
    "        coherenceScore = coherenceModel.get_coherence()\n",
    "        return model, coherenceScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a corpus and a model (like the one returned by the buildModel function).\n",
    "It returns a list of keywords found from running on the corpus\n",
    "'''\n",
    "\n",
    "def getKeywords(model, corpus):\n",
    "    n = len(corpus)\n",
    "    keywords = []\n",
    "\n",
    "    for i in range(n):    \n",
    "        for index, score in sorted(model[corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "            elements = model.print_topic(index, 5).split(\"+\")\n",
    "            keywords.append([x.strip().replace('\"', '').split(\"*\")[1] for x in elements])\n",
    "    keywords = keywords[:n]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this step we create a list of formats (basic/filtered/nouns) and model types (lda/lsa) which we will use to build our models.\n",
    "We use getKeywords() function and add new columns to the initial dataframe with these keywords.\n",
    "'''\n",
    "# dictionaries to store models and their coherence scores\n",
    "models = {'lda': [], 'lsi': []}\n",
    "coherenceScores = {'lda': [], 'lsi': []}\n",
    "\n",
    "lstFormats = ['basic', 'filtered', 'nouns']\n",
    "modelTypes = ['lda', 'lsi']\n",
    "\n",
    "# iterate through the list of TF-IDF corpus formats\n",
    "for lstFormat in lstFormats:\n",
    "    dictionary, corpus = getCorpus(df=df, column=\"text\", tfidfFormat=lstFormat)\n",
    "\n",
    "    # iterate through the list of model types\n",
    "    for modelType in modelTypes:\n",
    "        model, coherence = buildModel(dictionary=dictionary, corpus=corpus, modelType=modelType, num_topics =4)\n",
    "        \n",
    "        # save models and their coherence scores to the dictionaries\n",
    "        models[modelType].append(model)\n",
    "        coherenceScores[modelType].append(coherence)\n",
    "\n",
    "        # get keywords from the given text\n",
    "        kw = getKeywords(model, corpus)\n",
    "\n",
    "        # add keywords as new columns\n",
    "        colname = lstFormat + \"_\" + modelType\n",
    "        df[colname] = kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>basic_lda</th>\n",
       "      <th>basic_lsi</th>\n",
       "      <th>filtered_lda</th>\n",
       "      <th>filtered_lsi</th>\n",
       "      <th>nouns_lda</th>\n",
       "      <th>nouns_lsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>future hand viewer home theatre system plasma ...</td>\n",
       "      <td>[blair, tax, labour, party, election]</td>\n",
       "      <td>[labour, election, blair, tax, brown]</td>\n",
       "      <td>[film, holmes, domain, blog, google]</td>\n",
       "      <td>[labour, game, film, election, blair]</td>\n",
       "      <td>[search, film, game, music, people]</td>\n",
       "      <td>[election, tax, blair, party, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom bos left book alone former worldcom b...</td>\n",
       "      <td>[search, film, award, player, google]</td>\n",
       "      <td>[film, award, best, oscar, england]</td>\n",
       "      <td>[kilroysilk, film, phone, virus, software]</td>\n",
       "      <td>[mobile, phone, film, award, best]</td>\n",
       "      <td>[blair, bank, dollar, sale, profit]</td>\n",
       "      <td>[film, growth, economy, rate, bank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger wary farrell gamble leicester say rushed...</td>\n",
       "      <td>[mobile, film, holmes, blog, phone]</td>\n",
       "      <td>[labour, election, blair, brown, tax]</td>\n",
       "      <td>[blair, mobile, party, bank, tax]</td>\n",
       "      <td>[film, award, england, best, oscar]</td>\n",
       "      <td>[holmes, phone, player, film, camera]</td>\n",
       "      <td>[film, game, england, award, oscar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  future hand viewer home theatre system plasma ...   \n",
       "1  worldcom bos left book alone former worldcom b...   \n",
       "2  tiger wary farrell gamble leicester say rushed...   \n",
       "\n",
       "                               basic_lda  \\\n",
       "0  [blair, tax, labour, party, election]   \n",
       "1  [search, film, award, player, google]   \n",
       "2    [mobile, film, holmes, blog, phone]   \n",
       "\n",
       "                               basic_lsi  \\\n",
       "0  [labour, election, blair, tax, brown]   \n",
       "1    [film, award, best, oscar, england]   \n",
       "2  [labour, election, blair, brown, tax]   \n",
       "\n",
       "                                 filtered_lda  \\\n",
       "0        [film, holmes, domain, blog, google]   \n",
       "1  [kilroysilk, film, phone, virus, software]   \n",
       "2           [blair, mobile, party, bank, tax]   \n",
       "\n",
       "                            filtered_lsi  \\\n",
       "0  [labour, game, film, election, blair]   \n",
       "1     [mobile, phone, film, award, best]   \n",
       "2    [film, award, england, best, oscar]   \n",
       "\n",
       "                               nouns_lda                            nouns_lsi  \n",
       "0    [search, film, game, music, people]  [election, tax, blair, party, film]  \n",
       "1    [blair, bank, dollar, sale, profit]  [film, growth, economy, rate, bank]  \n",
       "2  [holmes, phone, player, film, camera]  [film, game, england, award, oscar]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us look at the new columns\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking through the created columns of keywords, I opine that the LSI model trained on a TF-IDF corpus that has been filtered to remove the top 10% of the most frequent words and words that appear less than 5 times in the documents does a better job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4719f_row0_col0, #T_4719f_row1_col0, #T_4719f_row1_col1, #T_4719f_row2_col0, #T_4719f_row2_col1 {\n",
       "  background-color: ;\n",
       "}\n",
       "#T_4719f_row0_col1 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4719f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4719f_level0_col0\" class=\"col_heading level0 col0\" >lda</th>\n",
       "      <th id=\"T_4719f_level0_col1\" class=\"col_heading level0 col1\" >lsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4719f_level0_row0\" class=\"row_heading level0 row0\" >basic</th>\n",
       "      <td id=\"T_4719f_row0_col0\" class=\"data row0 col0\" >0.475608</td>\n",
       "      <td id=\"T_4719f_row0_col1\" class=\"data row0 col1\" >0.635705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4719f_level0_row1\" class=\"row_heading level0 row1\" >filtered</th>\n",
       "      <td id=\"T_4719f_row1_col0\" class=\"data row1 col0\" >0.443679</td>\n",
       "      <td id=\"T_4719f_row1_col1\" class=\"data row1 col1\" >0.631448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4719f_level0_row2\" class=\"row_heading level0 row2\" >nouns</th>\n",
       "      <td id=\"T_4719f_row2_col0\" class=\"data row2 col0\" >0.389616</td>\n",
       "      <td id=\"T_4719f_row2_col1\" class=\"data row2 col1\" >0.436852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18db56c58e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for highlighting best model using coherence score\n",
    "def highlight_cells(val):\n",
    "    color = 'yellow' if val == maxVal else ''\n",
    "    return 'background-color: {}'.format(color)\n",
    "\n",
    "coherenceScoresDF = pd.DataFrame.from_dict(coherenceScores)\n",
    "coherenceScoresDF.set_index([pd.Index(lstFormats)], inplace=True)\n",
    "\n",
    "maxVal = coherenceScoresDF.max().max()\n",
    "coherenceScoresDF.style.applymap(highlight_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the coherence values, it is clear that the LDA model trained on the basically cleaned corpus performes better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Interactive for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the winning model is stored in the models list \n",
    "winningModel = models['lda'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echemochek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el608017086090975046394267984\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el608017086090975046394267984_data = {\"mdsDat\": {\"x\": [0.033463212404352675, 0.005344428540653148, -0.020794829679777593, -0.018012811265228232], \"y\": [-0.01583254568939633, 0.03417138010021454, -0.0060558107992178655, -0.012283023611600372], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [46.03783601556987, 39.54880245054234, 8.791633912172767, 5.6217276217150145]}, \"tinfo\": {\"Term\": [\"mobile\", \"phone\", \"game\", \"film\", \"camera\", \"search\", \"blog\", \"holmes\", \"award\", \"blair\", \"sale\", \"iraq\", \"google\", \"tax\", \"car\", \"virus\", \"domain\", \"oil\", \"people\", \"handset\", \"law\", \"kilroysilk\", \"p2p\", \"program\", \"technology\", \"walmart\", \"robot\", \"dallaglio\", \"kennedy\", \"mci\", \"linux\", \"uwb\", \"juninho\", \"crosby\", \"kennedy\", \"highdefinition\", \"lib\", \"miliband\", \"jaynes\", \"libya\", \"hdtv\", \"novartis\", \"koubek\", \"ibm\", \"korn\", \"wasted\", \"poster\", \"tax\", \"refugee\", \"dems\", \"tuition\", \"fee\", \"parking\", \"amr\", \"ppi\", \"contrack\", \"military\", \"workplace\", \"ferrari\", \"fiat\", \"conservative\", \"blair\", \"deficit\", \"inflation\", \"tory\", \"bush\", \"labour\", \"cut\", \"howard\", \"email\", \"spam\", \"economy\", \"asylum\", \"party\", \"tony\", \"brown\", \"spending\", \"virus\", \"election\", \"budget\", \"dollar\", \"price\", \"government\", \"prime\", \"service\", \"minister\", \"rate\", \"oil\", \"iraq\", \"bank\", \"mobile\", \"market\", \"would\", \"2004\", \"people\", \"dallaglio\", \"google\", \"ukip\", \"ebay\", \"woodward\", \"tindall\", \"piero\", \"kewell\", \"conte\", \"barcelona\", \"desailly\", \"timewarner\", \"hockney\", \"yahoo\", \"datamonitor\", \"marvel\", \"abbas\", \"jeeves\", \"gloucester\", \"toure\", \"lion\", \"davy\", \"clive\", \"bayern\", \"ruddock\", \"globe\", \"herlihy\", \"cml\", \"icann\", \"wenger\", \"search\", \"nomination\", \"palestinian\", \"rugby\", \"category\", \"goal\", \"wale\", \"kilroysilk\", \"award\", \"dance\", \"domain\", \"player\", \"cup\", \"woman\", \"film\", \"lending\", \"england\", \"actor\", \"information\", \"best\", \"december\", \"bbc\", \"oscar\", \"game\", \"show\", \"play\", \"sale\", \"british\", \"six\", \"new\", \"win\", \"get\", \"job\", \"people\", \"world\", \"bank\", \"p2p\", \"mci\", \"walmart\", \"cab\", \"daylewis\", \"aragones\", \"hendrix\", \"gatlin\", \"verizon\", \"henman\", \"mda\", \"mirza\", \"jesse\", \"dyer\", \"firstround\", \"casino\", \"directive\", \"hantuchova\", \"dechy\", \"seagrave\", \"leary\", \"projector\", \"mm\", \"qwest\", \"patenting\", \"capella\", \"alphabet\", \"ddos\", \"philippoussis\", \"sfar\", \"barclays\", \"berlin\", \"bot\", \"print\", \"holmes\", \"seed\", \"villa\", \"blog\", \"patent\", \"mobile\", \"invention\", \"film\", \"cinema\", \"phone\", \"olympic\", \"honour\", \"title\", \"network\", \"award\", \"law\", \"machine\", \"attack\", \"site\", \"game\", \"match\", \"technology\", \"open\", \"domain\", \"firm\", \"win\", \"mock\", \"bmw\", \"argonaut\", \"asimo\", \"cabir\", \"inbev\", \"interbrew\", \"musharraf\", \"panke\", \"bjorn\", \"crossover\", \"dome\", \"spyware\", \"mercedes\", \"petrel\", \"hansard\", \"milanesi\", \"manhunt\", \"cantona\", \"strain\", \"canvasser\", \"himrin\", \"khurmala\", \"ericsson\", \"pakistani\", \"alfaeco\", \"brewer\", \"robot\", \"iraqi\", \"honda\", \"bluetooth\", \"7e7\", \"holmes\", \"handset\", \"camera\", \"phone\", \"mobile\", \"game\", \"retail\", \"sale\", \"car\", \"iraq\", \"people\", \"music\", \"oil\", \"deal\", \"2005\", \"profit\", \"report\", \"market\"], \"Freq\": [10.0, 8.0, 11.0, 12.0, 4.0, 11.0, 4.0, 2.0, 9.0, 16.0, 9.0, 5.0, 6.0, 11.0, 4.0, 5.0, 4.0, 6.0, 10.0, 2.0, 6.0, 4.0, 1.0, 4.0, 7.0, 1.0, 1.0, 4.0, 6.0, 1.0, 5.016493857020434, 3.603722790559929, 3.2178748239022035, 3.061851204732112, 6.446937805244172, 4.349719727562834, 5.931410147772316, 2.6674909805673175, 2.622455648109997, 2.528250125973132, 2.9738527241749093, 2.5542664362488656, 2.6091749622868448, 3.923419911073421, 2.5281761476969518, 2.4792889249237966, 3.8758381406667204, 10.115309682941872, 2.7204731651351857, 3.997568337289011, 2.7488868999754215, 3.9587521999852977, 2.203871633722133, 2.1812698172043175, 2.188452447301109, 2.2109713024663504, 2.8267263137561542, 2.17559991075135, 2.02630078478199, 2.9586142561142688, 6.370090228780515, 14.562806498132309, 5.584146607888982, 3.125995918551211, 7.3513882835815645, 5.8321968247542975, 9.995490239510069, 5.767823570903428, 6.114557637695813, 6.254710010335792, 3.6177459012261775, 7.013215617008282, 3.772784816698682, 9.786932796967388, 5.001946029009563, 6.233750379445117, 5.682717376736748, 4.706459356208615, 8.279446286680631, 4.470267857015014, 7.40726460399686, 6.691905770074058, 6.2226309161189475, 6.121122152874304, 6.259759027685103, 6.6017273082426975, 5.567940609659154, 5.025870603526296, 4.575990739920647, 6.060850994437636, 5.829541004639422, 5.64568120959956, 5.622063116512717, 5.3732350709149905, 5.383019754772362, 4.3576764649381445, 6.251392061034513, 4.2534464087887915, 3.2932317714922754, 4.374600106150615, 2.39594764093382, 2.3073806457800226, 2.216430486730365, 3.094376036661878, 2.5160790744683132, 1.9601811292799558, 1.9858632382616983, 1.8975512416447582, 2.2889558382811503, 1.9212390963816457, 2.111368138961382, 1.8784033141294847, 1.8984848744719602, 2.327041293984891, 1.8316076213543642, 3.5222840086271447, 2.501202230590402, 2.3785721064530962, 1.81201460961054, 2.9186519715086727, 2.4894264503609804, 1.5627761798897672, 1.5011542570985235, 1.6237776856801218, 2.293733060536395, 9.556474512192937, 4.888113367967099, 1.976603398202335, 4.0864569797659005, 2.7533777395868304, 4.937105770434228, 5.303276406385295, 3.4495770826842937, 7.220490452430827, 2.556689322093726, 3.536831068015467, 6.7673860829825925, 4.716587302758449, 4.541438828428092, 7.762876312265636, 3.1956455223486016, 4.985460392849227, 3.3928058062930866, 4.730811840632866, 4.602722909366099, 4.090233273184884, 4.705773639838985, 3.698508084262501, 5.842895526421004, 4.91631050924694, 4.30116279721568, 4.957231929986754, 4.521743908056721, 3.749570888735157, 4.373196990035928, 4.002972251846018, 4.17987700448363, 4.186658354091298, 4.323954922779709, 4.068147018070009, 4.095323257330461, 1.0483237259397862, 0.9834908614737107, 1.0067816185077836, 0.7762259394459609, 0.585283891640263, 0.5866865298509636, 0.7216462089362717, 0.5140645328550407, 0.508128396401675, 0.8848791388895777, 0.4220028279872085, 0.6577737984622429, 0.40486826519628444, 0.42480840713628126, 0.4226868663879846, 0.5949916112420567, 0.6576369655547818, 0.37852053890936843, 0.4722325132230854, 0.3613142044330227, 0.735955389142954, 0.31808913296662694, 0.5229410127258526, 0.3154104723937779, 0.4768727229699099, 0.3074798399048689, 0.29615439408186306, 0.30080555247946894, 0.3674921193865927, 0.2829308457593408, 0.5116123378076854, 0.43837292772599473, 0.45959116066722766, 0.5716606923369147, 1.129402269071713, 0.5880193388788006, 0.46188745096382516, 1.1130322702421505, 0.6022189316684454, 1.6239134702688554, 0.5370478194842212, 1.4473392813834745, 0.7143155574647205, 1.0954872844107608, 0.634603527041377, 0.5617037774069682, 0.6748912319124284, 0.7938767285984959, 0.9089087532602513, 0.8197126180750359, 0.6775472131372304, 0.7255184994900435, 0.7022633634119689, 0.7869047589629112, 0.675175392286625, 0.6881288570716696, 0.6333678431414392, 0.6243131575814117, 0.6313664095967603, 0.6226769649722458, 0.5034238183355995, 0.5410613866751619, 0.5034760516907127, 0.45028810700910693, 0.5829048209027013, 0.3989304913591797, 0.3662570597817312, 0.36276700701907766, 0.3311231113249031, 0.32888369091407904, 0.3220154402634958, 0.48169993690752494, 0.5045744039708098, 0.40284031446363616, 0.25124342210813316, 0.24496528899824008, 0.22649878154524827, 0.2240551542034856, 0.3339716842853838, 0.20598707529834534, 0.18747156076294644, 0.17690532407379547, 0.17690532407379547, 0.3096543958051615, 0.30600015802448977, 0.16459017334470682, 0.24977475824681905, 0.5246174295816837, 0.45596689543837016, 0.18119012072352952, 0.29036564049026886, 0.3874197955980172, 0.6233185323804904, 0.5029157771725549, 0.7416940913962154, 1.1683498798833523, 0.9727769063846973, 0.869151995232604, 0.379062620093525, 0.6890387398129483, 0.49879011642405396, 0.4627380030177698, 0.5445837989530585, 0.42884994429189305, 0.40134642114975966, 0.39750628553575695, 0.3831028168780389, 0.37892460014715396, 0.3782878629658593, 0.378406944691445], \"Total\": [10.0, 8.0, 11.0, 12.0, 4.0, 11.0, 4.0, 2.0, 9.0, 16.0, 9.0, 5.0, 6.0, 11.0, 4.0, 5.0, 4.0, 6.0, 10.0, 2.0, 6.0, 4.0, 1.0, 4.0, 7.0, 1.0, 1.0, 4.0, 6.0, 1.0, 5.253865338546337, 3.8113106110094987, 3.434660285746965, 3.269797736722183, 6.9056242569782835, 4.68545498178531, 6.399679025799834, 2.8853780881276783, 2.8396971964336206, 2.737743632704438, 3.2238481075475725, 2.7717081330144335, 2.832270570078922, 4.2616414650988235, 2.74993594131997, 2.7036606452751872, 4.23570500958756, 11.055062599130586, 2.974961433393957, 4.379454403376118, 3.0127539753264827, 4.342243051037732, 2.4188459277611236, 2.396869025860817, 2.4105887936844796, 2.4374921538330194, 3.122073665108013, 2.4043235479037453, 2.24277752619341, 3.297765151477311, 7.189040532356919, 16.81547615887901, 6.476457095622477, 3.515935800029013, 8.74784904912033, 6.833072577432654, 12.29967986102489, 6.866396293458767, 7.454614022936084, 7.699710534551456, 4.147735970754427, 9.057824926039592, 4.377945852628833, 13.531003382869487, 6.124229097166016, 8.089682527799896, 7.249008398572203, 5.782755217294403, 11.634001165837464, 5.431772319015656, 10.386740755518806, 9.142216311699789, 8.783966549313778, 8.666138264700518, 9.156279262036545, 9.981889877772991, 7.798320107456324, 6.537459918020555, 5.713842522958651, 10.834477676271105, 10.036939569200207, 9.453036723935098, 10.047519666086798, 9.094324233559757, 10.820235535367592, 4.645871606529575, 6.689378426367248, 4.57266855871314, 3.5450774048641223, 4.718543775888646, 2.638110298836448, 2.5415544633980955, 2.4467405949171805, 3.436382903852106, 2.80875856789286, 2.1896031912630947, 2.2185844026314, 2.12993268522304, 2.5755038955166563, 2.162318786111112, 2.3786025109076916, 2.118472355685043, 2.144450615334415, 2.635174729409124, 2.0781101385372787, 4.002912515657916, 2.851886590502216, 2.713544611953519, 2.067930177257693, 3.34600202720951, 2.8562182348259326, 1.7963162004601563, 1.726605844627695, 1.874846202553076, 2.648651942049829, 11.035663222082805, 5.696315780564757, 2.286570071340414, 4.835234415114008, 3.2531973566206593, 6.247604141159824, 6.815625077374567, 4.2055891760785515, 9.844177397418374, 3.0126760124352145, 4.45071180982757, 10.281674018543196, 6.557125505924072, 6.2597265676784435, 12.418716023448377, 4.0615702820439425, 7.56952961778072, 4.473149697108811, 7.1834962087823095, 7.049748085982262, 5.953500721691575, 7.463331868415878, 5.344323257267111, 11.383782480752965, 8.68446174643685, 7.085305995318167, 9.971310350859838, 8.781866025042138, 5.78691860014534, 9.489966825643856, 7.1792554331417815, 8.554775301319015, 8.90392346734693, 10.820235535367592, 8.473877619955143, 10.834477676271105, 1.3834243436793299, 1.338434806321807, 1.403264125107543, 1.0998731620710203, 0.9071459811306251, 0.9307931661062255, 1.162157038311513, 0.8387812720080086, 0.8430824251821928, 1.4806642721014736, 0.754028918993496, 1.1794717773572958, 0.7327281376953854, 0.7693687102058328, 0.7671713308430937, 1.1167220277236911, 1.24909152996461, 0.7266653086290551, 0.9096038555666484, 0.697722005034896, 1.4487156034760866, 0.639232472639669, 1.0638943770799036, 0.6465194686761749, 0.9782007581843754, 0.6342115178730785, 0.616043284024515, 0.6285906471021242, 0.7801421482658741, 0.6032512147007227, 1.0948603557330145, 0.9387335627299221, 1.0021817438426677, 1.291469991524406, 2.8482504540030344, 1.420652654069219, 1.068859273422907, 4.332527298084937, 1.6483264159380726, 10.036939569200207, 1.3717714753044958, 12.418716023448377, 2.6397253671468017, 8.650956073524108, 2.409638819169207, 1.7753606541848437, 3.3360653559549798, 5.899588020883713, 9.844177397418374, 6.928086662678694, 3.6109890506265185, 5.164917434597609, 5.585488992785227, 11.383782480752965, 4.62355373332, 7.471237796469975, 5.211939085332583, 4.45071180982757, 8.16263191124297, 7.1792554331417815, 0.8416372123123503, 0.9307434691540405, 0.8673155589417979, 0.7910761775325375, 1.0264078016370093, 0.7332024351615398, 0.7042545800006266, 0.7206416552686681, 0.6731558544644267, 0.6886793772720398, 0.6868521297003861, 1.0453399005730253, 1.1613745295704676, 0.943919618609977, 0.5916395312318803, 0.6148898416744135, 0.5730570871764594, 0.5790889475091778, 0.9197747610909446, 0.5735557593948463, 0.5226510131057963, 0.5148690481258166, 0.5148690481258166, 0.9303085315504255, 0.9271460725905755, 0.504074167437309, 0.7661769026961923, 1.6093462333702973, 1.406005503532894, 0.5625738215851339, 0.982798683877327, 1.3839304705215634, 2.8482504540030344, 2.1518504545228794, 4.152105403302266, 8.650956073524108, 10.036939569200207, 11.383782480752965, 2.025938078979217, 9.971310350859838, 4.2194954259962, 5.713842522958651, 10.820235535367592, 8.96892891420931, 6.537459918020555, 5.877765965630419, 5.856971905683868, 7.203822170701272, 7.714942712351472, 9.453036723935098], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.5009, -7.8317, -7.9449, -7.9946, -7.25, -7.6435, -7.3334, -8.1325, -8.1495, -8.1861, -8.0238, -8.1759, -8.1546, -7.7467, -8.1861, -8.2057, -7.7589, -6.7996, -8.1128, -7.7279, -8.1024, -7.7377, -8.3234, -8.3337, -8.3304, -8.3202, -8.0745, -8.3363, -8.4074, -8.0289, -7.262, -6.4352, -7.3937, -7.9739, -7.1187, -7.3502, -6.8115, -7.3613, -7.303, -7.2803, -7.8278, -7.1658, -7.7858, -6.8326, -7.5038, -7.2836, -7.3762, -7.5647, -6.9998, -7.6162, -7.1112, -7.2127, -7.2854, -7.3019, -7.2795, -7.2263, -7.3966, -7.499, -7.5928, -7.3118, -7.3507, -7.3827, -7.3869, -7.4322, -7.4304, -7.4898, -7.1289, -7.514, -7.7698, -7.4859, -8.0879, -8.1256, -8.1658, -7.8321, -8.039, -8.2887, -8.2756, -8.3211, -8.1336, -8.3087, -8.2144, -8.3313, -8.3206, -8.1171, -8.3565, -7.7026, -8.0449, -8.0952, -8.3673, -7.8906, -8.0496, -8.5152, -8.5555, -8.4769, -8.1315, -6.7045, -7.3749, -8.2803, -7.554, -7.9489, -7.3649, -7.2934, -7.7234, -6.9848, -8.023, -7.6985, -7.0496, -7.4106, -7.4485, -6.9123, -7.7999, -7.3552, -7.74, -7.4076, -7.435, -7.5531, -7.4129, -7.6538, -7.1965, -7.3691, -7.5028, -7.3608, -7.4528, -7.6401, -7.4862, -7.5747, -7.5314, -7.5298, -7.4975, -7.5585, -7.5518, -7.4108, -7.4746, -7.4512, -7.7113, -7.9936, -7.9912, -7.7842, -8.1234, -8.135, -7.5803, -8.3207, -7.8769, -8.3622, -8.3141, -8.3191, -7.9772, -7.8771, -8.4294, -8.2082, -8.476, -7.7645, -8.6034, -8.1062, -8.6118, -8.1985, -8.6373, -8.6748, -8.6593, -8.459, -8.7205, -8.1281, -8.2826, -8.2354, -8.0172, -7.3363, -7.989, -8.2304, -7.3509, -7.9651, -6.9731, -8.0796, -7.0882, -7.7944, -7.3668, -7.9127, -8.0347, -7.8512, -7.6888, -7.5535, -7.6568, -7.8472, -7.7788, -7.8114, -7.6976, -7.8507, -7.8317, -7.9147, -7.9291, -7.9178, -7.9317, -7.6971, -7.625, -7.697, -7.8087, -7.5505, -7.9298, -8.0152, -8.0248, -8.1161, -8.1228, -8.144, -7.7412, -7.6948, -7.92, -8.3921, -8.4174, -8.4958, -8.5067, -8.1075, -8.5907, -8.6849, -8.7429, -8.7429, -8.1831, -8.195, -8.8151, -8.398, -7.6559, -7.7961, -8.719, -8.2474, -7.959, -7.4835, -7.6981, -7.3096, -6.8552, -7.0384, -7.151, -7.9809, -7.3833, -7.7064, -7.7814, -7.6185, -7.8574, -7.9237, -7.9333, -7.9702, -7.9812, -7.9829, -7.9826], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7295, 0.7197, 0.7105, 0.71, 0.707, 0.7014, 0.6997, 0.6972, 0.6961, 0.6961, 0.695, 0.694, 0.6937, 0.693, 0.6916, 0.6891, 0.6869, 0.6869, 0.6863, 0.6845, 0.684, 0.6832, 0.6826, 0.6815, 0.679, 0.6782, 0.6763, 0.6757, 0.6742, 0.6672, 0.6548, 0.6319, 0.6275, 0.6582, 0.6018, 0.6173, 0.5683, 0.6014, 0.5775, 0.5679, 0.639, 0.5199, 0.6269, 0.4518, 0.5733, 0.5151, 0.5323, 0.5698, 0.4356, 0.5809, 0.4376, 0.4637, 0.431, 0.428, 0.3954, 0.3623, 0.4388, 0.5128, 0.5536, 0.1948, 0.2324, 0.2603, 0.1951, 0.2495, 0.0775, 0.8636, 0.8599, 0.8553, 0.8539, 0.8519, 0.8314, 0.831, 0.8288, 0.8228, 0.8176, 0.817, 0.8168, 0.8121, 0.8097, 0.8094, 0.8085, 0.8074, 0.8058, 0.8033, 0.8014, 0.7997, 0.7964, 0.7959, 0.7955, 0.791, 0.7902, 0.7884, 0.7877, 0.7839, 0.7838, 0.7837, 0.7746, 0.782, 0.7594, 0.7608, 0.6922, 0.6767, 0.7295, 0.6177, 0.7635, 0.6978, 0.5094, 0.5982, 0.6067, 0.4578, 0.6879, 0.51, 0.6512, 0.5099, 0.5013, 0.5523, 0.4664, 0.5595, 0.2607, 0.3587, 0.4285, 0.2288, 0.2638, 0.4937, 0.1529, 0.3435, 0.2114, 0.173, 0.0104, 0.1938, -0.0453, 2.154, 2.1232, 2.0993, 2.0829, 1.9932, 1.9698, 1.9549, 1.9418, 1.925, 1.9166, 1.851, 1.8474, 1.8382, 1.8374, 1.8353, 1.8018, 1.7899, 1.7792, 1.7758, 1.7733, 1.7541, 1.7334, 1.7211, 1.7136, 1.7129, 1.7074, 1.6989, 1.6944, 1.6786, 1.6742, 1.6706, 1.6699, 1.6518, 1.6164, 1.5064, 1.5493, 1.5923, 1.0723, 1.4245, 0.6099, 1.4936, 0.2819, 1.1243, 0.3649, 1.0971, 1.2806, 0.8334, 0.4257, 0.049, 0.297, 0.7581, 0.4686, 0.3578, -0.2405, 0.5074, 0.0465, 0.3237, 0.4672, -0.1281, -0.0136, 2.3646, 2.3361, 2.3347, 2.315, 2.3127, 2.2699, 2.2247, 2.1921, 2.169, 2.1395, 2.121, 2.1038, 2.0449, 2.027, 2.0221, 1.9582, 1.9503, 1.929, 1.8655, 1.8545, 1.8532, 1.8102, 1.8102, 1.7785, 1.77, 1.7593, 1.7577, 1.7576, 1.7524, 1.7456, 1.6593, 1.6054, 1.3591, 1.4249, 1.1561, 0.8765, 0.5447, 0.3061, 1.2024, 0.2064, 0.7432, 0.365, -0.1106, -0.1619, 0.0881, 0.1848, 0.1514, -0.0665, -0.1367, -0.3396]}, \"token.table\": {\"Topic\": [1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 3, 4, 1, 1, 2, 3, 1, 2, 3, 1, 2, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 2, 3, 4, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 4, 2, 4, 1, 2, 3, 2, 1, 2, 3, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 3, 1, 2, 1, 2, 1, 2, 1, 2, 3, 1, 2, 2, 3, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 3, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 4, 1, 3, 3, 2, 1, 2, 2, 3, 4, 2, 3, 1, 2, 1, 2, 1, 1, 2, 2, 3, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 3, 1, 1, 1, 2, 1, 2, 3, 1, 3, 1, 2, 1, 1, 1, 2, 1, 2, 3, 1, 2, 2, 1, 2, 3, 3, 1, 1, 1, 2, 3, 3, 1, 2, 3, 4, 4, 1, 2, 1, 2, 3, 1, 2, 3, 2, 1, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 1, 2, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 2, 3, 1, 2, 3, 1, 1, 1, 2, 1, 2, 3, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 4, 2, 1, 2, 1, 2, 4, 1, 2, 1, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 1, 1, 2, 4, 1, 2, 1, 2, 3, 2, 2, 1, 2, 3, 1, 2, 1, 2, 2, 1, 2, 1, 3, 1, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 2, 1, 1, 2, 3, 1, 2, 3, 2], \"Freq\": [0.5497934614590785, 0.3298760768754471, 0.10995869229181571, 0.512210071741793, 0.3414733811611953, 0.7225796536029219, 0.944076515623574, 0.22355612213164763, 0.670668366394943, 0.8344218972422639, 1.0743525376140077, 1.1529828903566413, 0.9136705054490596, 0.5808418117014343, 0.3872278744676229, 0.19361393723381146, 0.10158289104605622, 0.7110802373223936, 0.10158289104605622, 0.5537876563390564, 0.3691917708927042, 1.0680875295916268, 0.9133584888371403, 0.9671506427031419, 0.2679768279451451, 0.6699420698628628, 0.2836980804997565, 0.7092452012493912, 0.14184904024987824, 0.8920353999062707, 0.1189380533208361, 0.6924364911274903, 0.23081216370916344, 1.074409902557691, 0.45548406097220173, 0.5693550762152522, 0.11387101524305043, 0.7416854715103122, 0.1236142452517187, 0.7364078913979367, 0.18410197284948418, 0.8780822875811368, 0.1463470479301895, 0.9091957459140445, 0.9742716281044516, 0.7225250104715623, 0.24084167015718744, 0.7109854845480051, 0.23699516151600172, 0.8954779928881537, 0.9221696906566792, 0.378827287279839, 0.378827287279839, 0.378827287279839, 0.7370433458841024, 1.158341961034689, 0.8346037239593789, 0.13910062065989648, 0.8730109780947487, 0.8205154617031067, 0.9174879431555781, 0.15250585017726814, 0.7625292508863407, 0.8738208142334962, 0.14563680237224938, 0.8609794541842634, 0.9957924408788423, 0.9249329991702846, 1.051935238235298, 1.1023584084599547, 0.5103980011354935, 0.340265334090329, 0.16796840157531195, 0.6718736063012478, 0.926432447773873, 0.15440540796231217, 0.9133557816965517, 0.9134075105390579, 0.8005818436926977, 0.6739361427000743, 0.288829775442889, 0.8987326456787522, 0.22468316141968805, 0.846243863641388, 0.7728124640471112, 0.2208035611563175, 0.6876396079013223, 0.2578648529629958, 0.7792500735028642, 0.12987501225047737, 0.2642172104462116, 0.660543026115529, 0.9211828893465691, 0.8917514005031663, 0.9097069870654918, 0.24157086725677238, 0.644188979351393, 0.08052362241892412, 0.49003802247783795, 0.36752851685837845, 0.12250950561945949, 0.3513770582636278, 0.5270655873954417, 0.08784426456590695, 0.08784426456590695, 1.1922059222972876, 0.4675751097031457, 0.4675751097031457, 0.7002266058013199, 0.7589629551617828, 0.16006135750693656, 0.8003067875346829, 0.8969443224126844, 0.6830627104868395, 0.22768757016227983, 0.46471630865339375, 0.46471630865339375, 0.9305649335576617, 0.860468909995925, 0.6753725465265144, 1.1133897247531737, 0.8537057800256296, 0.9389968114370555, 0.35109272030293676, 0.35109272030293676, 0.35109272030293676, 0.5632658342646157, 0.5632658342646157, 0.8048706454203288, 0.1341451075700548, 0.9386054722712915, 1.066754167502644, 0.8532579007771542, 0.2784159609571263, 0.6960399023928157, 0.7289843957267207, 0.7289843957267207, 0.8750678689357682, 0.17501357378715363, 0.7112347693428531, 1.0564506679682975, 0.9326398032664004, 0.44924015965198605, 0.44924015965198605, 0.873448827661151, 0.868857003613666, 0.817413993193544, 0.7133364373924207, 0.23777881246414026, 1.0909345032088271, 1.0592208356408561, 0.8130292912491085, 0.1626058582498217, 0.5773599833194681, 0.28867999165973407, 0.14433999582986703, 0.6902666041565187, 0.6902666041565187, 0.24621019225518867, 0.738630576765566, 0.937547020063263, 1.095792887311547, 0.9516802730584303, 0.9992724008714847, 0.5538648752349424, 0.2769324376174712, 0.2769324376174712, 0.6347166709728308, 0.3173583354864154, 0.8408298531715523, 0.21628384954054328, 0.6488515486216299, 0.21628384954054328, 0.7471413589042339, 1.0397250926469397, 0.9608998126878632, 0.7012700085569101, 0.30054428938153294, 0.84783715829181, 0.9399429318770571, 0.5977917829067999, 0.19926392763559997, 0.19926392763559997, 0.09963196381779998, 1.188160391877822, 0.5574801682371004, 0.3344881009422602, 0.5085100839889872, 0.33900672265932486, 0.16950336132966243, 0.4214977853443251, 0.4214977853443251, 0.10537444633608127, 0.8777603266060996, 1.0823650456793525, 0.7648230448369503, 0.15296460896739006, 0.4149999543685884, 0.4149999543685884, 0.4149999543685884, 0.5756015085522752, 0.19186716951742505, 0.19186716951742505, 0.18711442999639266, 0.7484577199855706, 0.7228440099155827, 0.8746725171766011, 0.8268405924684893, 0.7390434927139398, 0.2217130478141819, 0.6066759534584623, 0.6066759534584623, 0.4620971497022164, 0.3696777197617731, 0.09241942994044328, 0.09241942994044328, 0.46237663976145177, 0.23118831988072588, 0.11559415994036294, 0.11559415994036294, 0.786919984915834, 0.28227432962268123, 0.5645486592453625, 0.14113716481134062, 0.29178127944821464, 0.6808229853791674, 0.09726042648273821, 0.9443528269664578, 0.8296728190389898, 0.7656786671128882, 0.21876533346082522, 0.692349904505862, 0.230783301501954, 0.7743114486304363, 0.4164455935907644, 0.5552607914543525, 0.6520711076407303, 0.7693964747950177, 0.25646549159833926, 1.0084164340165842, 0.6480929523942023, 0.2592371809576809, 0.49359850154149676, 0.49359850154149676, 0.6213703299294381, 0.6213703299294381, 0.8965924035921556, 0.20681520566494013, 0.8272608226597605, 0.4011508878223889, 0.5014386097779862, 0.10028772195559722, 0.09061530601975601, 0.9061530601975601, 0.703901827892743, 0.703901827892743, 0.6552880081843939, 0.21842933606146464, 0.3454445523041046, 0.575740920506841, 0.35807070832713106, 0.35807070832713106, 0.17903535416356553, 0.34560707315803085, 0.6912141463160617, 0.9643815392792334, 0.8276994135062372, 0.13794990225103954, 0.8610486751158977, 0.9045629466436887, 0.09045629466436887, 0.5353865194720382, 0.2676932597360191, 0.13384662986800955, 0.9014757327365399, 0.7581184156258023, 0.29975431932559987, 0.29975431932559987, 0.29975431932559987, 0.8164292877798689, 0.16328585755597377, 0.8001967067211693, 0.11431381524588133, 0.9624128976184785, 0.9957666721441798, 0.8747627230445273, 1.0495077437261202, 1.1861236459577447, 0.8646397456089049, 0.17292794912178097, 0.14672168563374205, 0.7336084281687102, 0.7126242181409449, 0.7397378082545687, 0.7551011019032466, 0.2785804208563674, 0.5571608417127348, 0.1392902104281837, 0.15975138677197395, 0.7987569338598698, 0.8477191671802764, 0.8318348009957884, 0.4720389152871875, 0.4720389152871875, 0.11800972882179687, 0.5971623046682543, 0.3981082031121695, 0.09952705077804237, 0.7765470685101767], \"Term\": [\"2004\", \"2004\", \"2004\", \"2005\", \"2005\", \"7e7\", \"abbas\", \"actor\", \"actor\", \"amr\", \"aragones\", \"argonaut\", \"asylum\", \"attack\", \"attack\", \"attack\", \"award\", \"award\", \"award\", \"bank\", \"bank\", \"barcelona\", \"barclays\", \"bayern\", \"bbc\", \"bbc\", \"best\", \"best\", \"best\", \"blair\", \"blair\", \"blog\", \"blog\", \"bmw\", \"british\", \"british\", \"british\", \"brown\", \"brown\", \"budget\", \"budget\", \"bush\", \"bush\", \"cab\", \"cabir\", \"camera\", \"camera\", \"car\", \"car\", \"casino\", \"category\", \"cinema\", \"cinema\", \"cinema\", \"clive\", \"cml\", \"conservative\", \"conservative\", \"conte\", \"contrack\", \"crosby\", \"cup\", \"cup\", \"cut\", \"cut\", \"dallaglio\", \"dance\", \"datamonitor\", \"davy\", \"daylewis\", \"deal\", \"deal\", \"december\", \"december\", \"deficit\", \"deficit\", \"dems\", \"desailly\", \"directive\", \"dollar\", \"dollar\", \"domain\", \"domain\", \"ebay\", \"economy\", \"economy\", \"election\", \"election\", \"email\", \"email\", \"england\", \"england\", \"fee\", \"ferrari\", \"fiat\", \"film\", \"film\", \"film\", \"firm\", \"firm\", \"firm\", \"game\", \"game\", \"game\", \"game\", \"gatlin\", \"get\", \"get\", \"globe\", \"gloucester\", \"goal\", \"goal\", \"google\", \"government\", \"government\", \"handset\", \"handset\", \"hdtv\", \"hendrix\", \"henman\", \"herlihy\", \"highdefinition\", \"hockney\", \"holmes\", \"holmes\", \"holmes\", \"honour\", \"honour\", \"howard\", \"howard\", \"ibm\", \"icann\", \"inflation\", \"information\", \"information\", \"invention\", \"invention\", \"iraq\", \"iraq\", \"iraqi\", \"jaynes\", \"jeeves\", \"job\", \"job\", \"juninho\", \"kennedy\", \"kewell\", \"kilroysilk\", \"kilroysilk\", \"korn\", \"koubek\", \"labour\", \"labour\", \"law\", \"law\", \"law\", \"leary\", \"leary\", \"lending\", \"lending\", \"lib\", \"libya\", \"linux\", \"lion\", \"machine\", \"machine\", \"machine\", \"market\", \"market\", \"marvel\", \"match\", \"match\", \"match\", \"mci\", \"miliband\", \"military\", \"minister\", \"minister\", \"mirza\", \"mm\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mock\", \"music\", \"music\", \"network\", \"network\", \"network\", \"new\", \"new\", \"new\", \"nomination\", \"novartis\", \"oil\", \"oil\", \"olympic\", \"olympic\", \"olympic\", \"open\", \"open\", \"open\", \"oscar\", \"oscar\", \"p2p\", \"palestinian\", \"parking\", \"party\", \"party\", \"patent\", \"patent\", \"people\", \"people\", \"people\", \"people\", \"phone\", \"phone\", \"phone\", \"phone\", \"piero\", \"play\", \"play\", \"play\", \"player\", \"player\", \"player\", \"poster\", \"ppi\", \"price\", \"price\", \"prime\", \"prime\", \"print\", \"profit\", \"profit\", \"program\", \"rate\", \"rate\", \"refugee\", \"report\", \"report\", \"retail\", \"retail\", \"robot\", \"robot\", \"ruddock\", \"rugby\", \"rugby\", \"sale\", \"sale\", \"sale\", \"search\", \"search\", \"seed\", \"seed\", \"service\", \"service\", \"show\", \"show\", \"site\", \"site\", \"site\", \"six\", \"six\", \"spam\", \"spending\", \"spending\", \"spyware\", \"tax\", \"tax\", \"technology\", \"technology\", \"technology\", \"timewarner\", \"tindall\", \"title\", \"title\", \"title\", \"tony\", \"tony\", \"tory\", \"tory\", \"toure\", \"tuition\", \"ukip\", \"uwb\", \"verizon\", \"virus\", \"virus\", \"wale\", \"wale\", \"walmart\", \"wasted\", \"wenger\", \"win\", \"win\", \"win\", \"woman\", \"woman\", \"woodward\", \"workplace\", \"world\", \"world\", \"world\", \"would\", \"would\", \"would\", \"yahoo\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el608017086090975046394267984\", ldavis_el608017086090975046394267984_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el608017086090975046394267984\", ldavis_el608017086090975046394267984_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el608017086090975046394267984\", ldavis_el608017086090975046394267984_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.033463 -0.015833       1        1  46.037836\n",
       "1      0.005344  0.034171       2        1  39.548802\n",
       "3     -0.020795 -0.006056       3        1   8.791634\n",
       "2     -0.018013 -0.012283       4        1   5.621728, topic_info=        Term       Freq      Total Category  logprob  loglift\n",
       "142   mobile  10.000000  10.000000  Default  30.0000  30.0000\n",
       "338    phone   8.000000   8.000000  Default  29.0000  29.0000\n",
       "397     game  11.000000  11.000000  Default  28.0000  28.0000\n",
       "629     film  12.000000  12.000000  Default  27.0000  27.0000\n",
       "2713  camera   4.000000   4.000000  Default  26.0000  26.0000\n",
       "...      ...        ...        ...      ...      ...      ...\n",
       "1483    deal   0.397506   5.877766   Topic4  -7.9333   0.1848\n",
       "2259    2005   0.383103   5.856972   Topic4  -7.9702   0.1514\n",
       "1225  profit   0.378925   7.203822   Topic4  -7.9812  -0.0665\n",
       "2367  report   0.378288   7.714943   Topic4  -7.9829  -0.1367\n",
       "137   market   0.378407   9.453037   Topic4  -7.9826  -0.3396\n",
       "\n",
       "[271 rows x 6 columns], token_table=       Topic      Freq   Term\n",
       "term                         \n",
       "1190       1  0.549793   2004\n",
       "1190       2  0.329876   2004\n",
       "1190       3  0.109959   2004\n",
       "2259       1  0.512210   2005\n",
       "2259       2  0.341473   2005\n",
       "...      ...       ...    ...\n",
       "1025       3  0.118010  world\n",
       "451        1  0.597162  would\n",
       "451        2  0.398108  would\n",
       "451        3  0.099527  would\n",
       "13243      2  0.776547  yahoo\n",
       "\n",
       "[309 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 4, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interacting with LDA output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "d, c = getCorpus(df=df, column=\"text\", tfidfFormat=\"basic\")\n",
    "\n",
    "vis = gensimvis.prepare(winningModel, c, d)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidently, there is an overlap among many of the topics as seen in the visualization above. It is important to objectively select the best number of topics on which to base the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
