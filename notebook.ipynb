{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Tagging: BBC News Articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus used in this project includes 2,225 documents from BBC's news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. \n",
    "\n",
    "The CSV file includes two columns: category (the five class labels) and text (pre-processed article content). In this project, I will use only the text column.\n",
    "\n",
    "More information on this data set as well as a paper written using this data set is available here http://mlg.ucd.ie/datasets/bbc.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  tv future in the hands of viewers with home th...\n",
       "1  worldcom boss  left books alone  former worldc...\n",
       "2  tigers wary of farrell  gamble  leicester say ...\n",
       "3  yeading face newcastle in fa cup premiership s...\n",
       "4  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/BBC-articles.csv\")\n",
    "df = df[['text']]#[:100]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a string and does basic text preprocessing on it.\n",
    "It returns a string\n",
    "'''\n",
    "\n",
    "def preprocess(text):\n",
    "    import contractions\n",
    "    import string\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    # load the text and convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # expand contractions\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    text = ' '.join(expanded_words)\n",
    "\n",
    "    # remove punctuations: using translate\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # tokenize\n",
    "    tokens_raw = text.split(\" \")\n",
    "\n",
    "    # limit to tokens with more than 2 characters\n",
    "    tokens_raw = [token for token in tokens_raw if len(token) > 2]\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_filtered = [lemmatizer.lemmatize(token) for token in tokens_raw if not token in stop_words]\n",
    "    text = ' '.join(tokens_filtered)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text column using the defined function\n",
    "df['text'] = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitText = df['text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a string of text and returns a list of nouns, noun phrases and named entities.\n",
    "The function has a high complexity, and there may be more efficient ways to go about it.\n",
    "However, this gives me the output I desire more compared to available methods/packages.\n",
    "'''\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "def getNouns(text):\n",
    "        from nltk import ne_chunk, pos_tag, sent_tokenize, word_tokenize\n",
    "        from nltk.tree import Tree\n",
    "        \n",
    "        global nouns\n",
    "        nouns = []\n",
    "\n",
    "        for sentence in sent_tokenize(text):\n",
    "                for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "                        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                                nouns.append(word)\n",
    "\n",
    "                chunked = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "                continuous_chunk = []\n",
    "                current_chunk = []\n",
    "                \n",
    "                for i in chunked:\n",
    "                        if type(i) == Tree:\n",
    "                                current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "                        if current_chunk:\n",
    "                                named_entity = \" \".join(current_chunk)\n",
    "                                if named_entity not in nouns:\n",
    "                                        nouns.append(named_entity)\n",
    "                                        current_chunk = []\n",
    "                        else:\n",
    "                                continue\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dataframe, a column name (str) and a a TFIDF data format (basic, filtered & nouns)\n",
    "basic -> only basic preprocessing done before building tfidf\n",
    "filtered -> tfidf filtered to remove top 10% of the most frequent words and words that appear less than 5 times in the documents\n",
    "nouns -> tfidf built on text limited to nouns, noun phrases, and named entity recognition.\n",
    "\n",
    "It returns a dictionary and a TF-IDF corpus.\n",
    "'''\n",
    "\n",
    "def getCorpus(df=df, column=\"text\", tfidfFormat=\"basic\"):\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models import TfidfModel\n",
    "\n",
    "    # TF-IDF with basic cleaning\n",
    "    if tfidfFormat==\"basic\":\n",
    "        tokens = df[column].apply(lambda x: x.split())\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "    \n",
    "    # TF-IDF with term frequency filter cleaning    \n",
    "    elif tfidfFormat==\"filtered\":\n",
    "        tokens = df[column].apply(lambda x: x.split())\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dictionary.filter_extremes(no_below=5, no_above=0.90)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus\n",
    "\n",
    "    # TF-IDF with only nouns, noun phrases and NER\n",
    "    elif tfidfFormat==\"nouns\":\n",
    "        tokens = df[column].apply(lambda x: getNouns(x))\n",
    "        dictionary = Dictionary(tokens)\n",
    "        dtm = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "        vectorizer = TfidfModel(dtm)\n",
    "        tfidfCorpus = vectorizer[dtm]\n",
    "        return dictionary, tfidfCorpus               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a dictionary, a corpus, the type of model (lda or lsi) and the number of topics.\n",
    "It builds a model using these parameters and returns the model and its coherence score.\n",
    "'''\n",
    "\n",
    "def buildModel(dictionary, corpus, modelType:str, num_topics):\n",
    "    from gensim.models import LsiModel,LdaModel,CoherenceModel\n",
    "\n",
    "    if modelType==\"lda\":\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        coherenceModel = CoherenceModel(model=model, texts=splitText, dictionary=dictionary, coherence='c_v')\n",
    "        coherenceScore = coherenceModel.get_coherence()\n",
    "        return model, coherenceScore\n",
    "    \n",
    "    elif modelType==\"lsi\":\n",
    "        model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        coherenceModel = CoherenceModel(model=model, texts=splitText, dictionary=dictionary, coherence='c_v')\n",
    "        coherenceScore = coherenceModel.get_coherence()\n",
    "        return model, coherenceScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a corpus and a model (like the one returned by the buildModel function).\n",
    "It returns a list of keywords found from running on the corpus\n",
    "'''\n",
    "\n",
    "def getKeywords(model, corpus):\n",
    "    n = len(corpus)\n",
    "    keywords = []\n",
    "\n",
    "    for i in range(n):    \n",
    "        for index, score in sorted(model[corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "            elements = model.print_topic(index, 5).split(\"+\")\n",
    "            keywords.append([x.strip().replace('\"', '').split(\"*\")[1] for x in elements])\n",
    "    keywords = keywords[:n]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echemochek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n",
      "C:\\Users\\echemochek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n",
      "C:\\Users\\echemochek\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this step we create a list of formats (basic/filtered/nouns) and model types (lda/lsa) which we will use to build our models.\n",
    "We use getKeywords() function and add new columns to the initial dataframe with these keywords.\n",
    "'''\n",
    "# dictionaries to store models and their coherence scores\n",
    "models = {'lda': [], 'lsi': []}\n",
    "coherenceScores = {'lda': [], 'lsi': []}\n",
    "\n",
    "lstFormats = ['basic', 'filtered', 'nouns']\n",
    "modelTypes = ['lda', 'lsi']\n",
    "\n",
    "# iterate through the list of TF-IDF corpus formats\n",
    "for lstFormat in lstFormats:\n",
    "    dictionary, corpus = getCorpus(df=df, column=\"text\", tfidfFormat=lstFormat)\n",
    "\n",
    "    # iterate through the list of model types\n",
    "    for modelType in modelTypes:\n",
    "        model, coherence = buildModel(dictionary=dictionary, corpus=corpus, modelType=modelType, num_topics =4)\n",
    "        \n",
    "        # save models and their coherence scores to the dictionaries\n",
    "        models[modelType].append(model)\n",
    "        coherenceScores[modelType].append(coherence)\n",
    "\n",
    "        # get keywords from the given text\n",
    "        kw = getKeywords(model, corpus)\n",
    "\n",
    "        # add keywords as new columns\n",
    "        colname = lstFormat + \"_\" + modelType\n",
    "        df[colname] = kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>basic_lda</th>\n",
       "      <th>basic_lsi</th>\n",
       "      <th>filtered_lda</th>\n",
       "      <th>filtered_lsi</th>\n",
       "      <th>nouns_lda</th>\n",
       "      <th>nouns_lsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>future hand viewer home theatre system plasma ...</td>\n",
       "      <td>[holmes, mobile, email, music, british]</td>\n",
       "      <td>[labour, election, blair, tax, brown]</td>\n",
       "      <td>[film, award, bank, game, dollar]</td>\n",
       "      <td>[mobile, phone, film, award, best]</td>\n",
       "      <td>[blair, party, election, tax, phone]</td>\n",
       "      <td>[film, growth, economy, rate, bank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom bos left book alone former worldcom b...</td>\n",
       "      <td>[mobile, phone, search, blog, people]</td>\n",
       "      <td>[mobile, phone, film, award, best]</td>\n",
       "      <td>[search, phone, mobile, virus, user]</td>\n",
       "      <td>[film, award, england, best, oscar]</td>\n",
       "      <td>[search, sale, virus, film, figure]</td>\n",
       "      <td>[film, game, england, award, oscar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger wary farrell gamble leicester say rushed...</td>\n",
       "      <td>[blair, labour, party, election, oil]</td>\n",
       "      <td>[film, award, best, oscar, england]</td>\n",
       "      <td>[price, profit, game, 2004, sale]</td>\n",
       "      <td>[labour, election, blair, brown, tax]</td>\n",
       "      <td>[award, film, nomination, game, band]</td>\n",
       "      <td>[election, tax, blair, party, film]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  future hand viewer home theatre system plasma ...   \n",
       "1  worldcom bos left book alone former worldcom b...   \n",
       "2  tiger wary farrell gamble leicester say rushed...   \n",
       "\n",
       "                                 basic_lda  \\\n",
       "0  [holmes, mobile, email, music, british]   \n",
       "1    [mobile, phone, search, blog, people]   \n",
       "2    [blair, labour, party, election, oil]   \n",
       "\n",
       "                               basic_lsi  \\\n",
       "0  [labour, election, blair, tax, brown]   \n",
       "1     [mobile, phone, film, award, best]   \n",
       "2    [film, award, best, oscar, england]   \n",
       "\n",
       "                           filtered_lda  \\\n",
       "0     [film, award, bank, game, dollar]   \n",
       "1  [search, phone, mobile, virus, user]   \n",
       "2     [price, profit, game, 2004, sale]   \n",
       "\n",
       "                            filtered_lsi  \\\n",
       "0     [mobile, phone, film, award, best]   \n",
       "1    [film, award, england, best, oscar]   \n",
       "2  [labour, election, blair, brown, tax]   \n",
       "\n",
       "                               nouns_lda                            nouns_lsi  \n",
       "0   [blair, party, election, tax, phone]  [film, growth, economy, rate, bank]  \n",
       "1    [search, sale, virus, film, figure]  [film, game, england, award, oscar]  \n",
       "2  [award, film, nomination, game, band]  [election, tax, blair, party, film]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us look at the new columns\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking through the created columns of keywords, I opine that the LSI model trained on a TF-IDF corpus that has been filtered to remove the top 10% of the most frequent words and words that appear less than 5 times in the documents does a better job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_96263_row0_col0, #T_96263_row0_col1, #T_96263_row1_col0, #T_96263_row2_col0, #T_96263_row2_col1 {\n",
       "  background-color: ;\n",
       "}\n",
       "#T_96263_row1_col1 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_96263\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_96263_level0_col0\" class=\"col_heading level0 col0\" >lda</th>\n",
       "      <th id=\"T_96263_level0_col1\" class=\"col_heading level0 col1\" >lsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_96263_level0_row0\" class=\"row_heading level0 row0\" >basic</th>\n",
       "      <td id=\"T_96263_row0_col0\" class=\"data row0 col0\" >0.353226</td>\n",
       "      <td id=\"T_96263_row0_col1\" class=\"data row0 col1\" >0.650648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_96263_level0_row1\" class=\"row_heading level0 row1\" >filtered</th>\n",
       "      <td id=\"T_96263_row1_col0\" class=\"data row1 col0\" >0.437494</td>\n",
       "      <td id=\"T_96263_row1_col1\" class=\"data row1 col1\" >0.700128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_96263_level0_row2\" class=\"row_heading level0 row2\" >nouns</th>\n",
       "      <td id=\"T_96263_row2_col0\" class=\"data row2 col0\" >0.342381</td>\n",
       "      <td id=\"T_96263_row2_col1\" class=\"data row2 col1\" >0.656276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f4fd4e7fa0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for highlighting best model using coherence score\n",
    "def highlight_cells(val):\n",
    "    color = 'yellow' if val == maxVal else ''\n",
    "    return 'background-color: {}'.format(color)\n",
    "\n",
    "coherenceScoresDF = pd.DataFrame.from_dict(coherenceScores)\n",
    "coherenceScoresDF.set_index([pd.Index(lstFormats)], inplace=True)\n",
    "\n",
    "maxVal = coherenceScoresDF.max().max()\n",
    "coherenceScoresDF.style.applymap(highlight_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the coherence values, it is clear that the LDA model trained on the basically cleaned corpus performes better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Interactive for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the winning model is stored in the models list \n",
    "winningModel = models['lda'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 8328 is out of bounds for axis 1 with size 8328",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\echemochek\\OneDrive\\Desktop\\vscodee\\BBC-News-Articles\\notebook.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echemochek/OneDrive/Desktop/vscodee/BBC-News-Articles/notebook.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echemochek/OneDrive/Desktop/vscodee/BBC-News-Articles/notebook.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m d, c \u001b[39m=\u001b[39m getCorpus(df\u001b[39m=\u001b[39mdf, column\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, tfidfFormat\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbasic\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/echemochek/OneDrive/Desktop/vscodee/BBC-News-Articles/notebook.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m vis \u001b[39m=\u001b[39m gensimvis\u001b[39m.\u001b[39;49mprepare(winningModel, c, d)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/echemochek/OneDrive/Desktop/vscodee/BBC-News-Articles/notebook.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m vis\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyLDAvis\\gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     78\u001b[0m     \u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m pyLDAvis\u001b[39m.\u001b[39mprepare(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyLDAvis\\gensim_models.py:49\u001b[0m, in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     47\u001b[0m         gamma \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39minference(corpus)\n\u001b[0;32m     48\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m         gamma, _ \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39;49minference(corpus)\n\u001b[0;32m     50\u001b[0m     doc_topic_dists \u001b[39m=\u001b[39m gamma \u001b[39m/\u001b[39m gamma\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[:, \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\gensim\\models\\ldamodel.py:705\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    703\u001b[0m Elogthetad \u001b[39m=\u001b[39m Elogtheta[d, :]\n\u001b[0;32m    704\u001b[0m expElogthetad \u001b[39m=\u001b[39m expElogtheta[d, :]\n\u001b[1;32m--> 705\u001b[0m expElogbetad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpElogbeta[:, ids]\n\u001b[0;32m    707\u001b[0m \u001b[39m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39m# phinorm is the normalizer.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[39m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[0;32m    710\u001b[0m phinorm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[39m+\u001b[39m epsilon\n",
      "\u001b[1;31mIndexError\u001b[0m: index 8328 is out of bounds for axis 1 with size 8328"
     ]
    }
   ],
   "source": [
    "# interacting with LDA output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "d, c = getCorpus(df=df, column=\"text\", tfidfFormat=\"basic\")\n",
    "\n",
    "vis = gensimvis.prepare(winningModel, c, d)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidently, there is an overlap among many of the topics as seen in the visualization above. It is important to objectively select the best number of topics on which to base the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
