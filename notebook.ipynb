{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Tagging: BBC News Articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus used in this project includes 2,225 documents from BBC's news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. \n",
    "\n",
    "The CSV file includes two columns: category (the five class labels) and text (pre-processed article content). In this project, I will use only the text column.\n",
    "\n",
    "More information on this data set as well as a paper written using this data set is available here http://mlg.ucd.ie/datasets/bbc.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  tv future in the hands of viewers with home th...\n",
       "1  worldcom boss  left books alone  former worldc...\n",
       "2  tigers wary of farrell  gamble  leicester say ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/BBC-articles.csv\")\n",
    "df = df[['text']][:100]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes as input a df and name of column (containing sentences) in the df.\n",
    "The input is split to tokens which are lemmatized, and stopwords removed.\n",
    "The output is a list of lists.  \n",
    "'''\n",
    "import re\n",
    "def preprocess_text(text):    \n",
    "    cleanTokens=[]\n",
    "    lem = WordNetLemmatizer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    for txt in text:\n",
    "        words = [lem.lemmatize(w) for w in word_tokenize(txt) if (w not in stop) and len(w)>2]\n",
    "        cleanTokens.append(words)        \n",
    "\n",
    "    return cleanTokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleanTokens</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[future, hand, viewer, home, theatre, system, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, bos, left, book, alone, former, wor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tiger, wary, farrell, gamble, leicester, say,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, cup, premiership, s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raid, box, office, ocean, twel...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                         cleanTokens cleanText  \n",
       "0  [future, hand, viewer, home, theatre, system, ...       NaN  \n",
       "1  [worldcom, bos, left, book, alone, former, wor...       NaN  \n",
       "2  [tiger, wary, farrell, gamble, leicester, say,...       NaN  \n",
       "3  [yeading, face, newcastle, cup, premiership, s...       NaN  \n",
       "4  [ocean, twelve, raid, box, office, ocean, twel...       NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['cleanTokens'] = preprocess_text(df.text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data (options: TF-IDF, TF-IDF ngrams, word2vec, doc2vec)\n",
    "def vectorizeStep(inputData, fittingData=df.text, outputFormat=\"tfidfUnfiltered\"):\n",
    "    \n",
    "    # TF-IDF input\n",
    "    if outputFormat == \"tfidf\":\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "                            strip_accents=\"unicode\", lowercase=True, analyzer='word', \n",
    "                            stop_words='english', max_df=0.9, min_df=5\n",
    "                                )\n",
    "        vectorizer.fit(fittingData)\n",
    "        \n",
    "        transformedData = vectorizer.transform(inputData)\n",
    "    \n",
    "    # TF-IDF ngrams input\n",
    "    elif outputFormat == \"tfidfFiltered\":\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "                            strip_accents=\"unicode\", lowercase=True, analyzer='word', ngram_range=(2,3), \n",
    "                            max_df=0.9, min_df=5\n",
    "                            )\n",
    "        vectorizer.fit(fittingData)\n",
    "        \n",
    "        transformedData = vectorizer.transform(inputData)\n",
    "        \n",
    "    return transformedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting tokens to use with Gensim\n",
    "\n",
    "tokens = []\n",
    "for i, row in df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    words = text.split()\n",
    "    tokens.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# TF-IDF with basic cleaning\n",
    "dictionaryLtd = Dictionary(tokens)\n",
    "dtmLtd = [dictionaryLtd.doc2bow(doc) for doc in tokens]\n",
    "tfidfVectorizerLtd = TfidfModel(dtmLtd) \n",
    "tfidfLtd = tfidfVectorizerLtd[dtmLtd]\n",
    "\n",
    "# TF-IDF with term frequency filter cleaning\n",
    "dictionaryFiltered = Dictionary(tokens)\n",
    "dictionaryFiltered.filter_extremes(no_below=5, no_above=0.90)\n",
    "dtmFiltered = [dictionaryFiltered.doc2bow(doc) for doc in tokens]\n",
    "tfidfVectorizerFiltered = TfidfModel(dtmFiltered) \n",
    "tfidfFiltered = tfidfVectorizerFiltered[dtmFiltered]\n",
    "\n",
    "# TF-IDF with only nouns, noun phrases and NER\n",
    "dictionaryNouns = Dictionary(tokens)\n",
    "dtmNouns = [dictionaryNouns.doc2bow(doc) for doc in tokens]\n",
    "tfidfVectorizerNouns = TfidfModel(dtmNouns) \n",
    "tfidfNouns = tfidfVectorizerNouns[dtmNouns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, LdaMulticore\n",
    "\n",
    "# tfidfLtd\n",
    "lda_Ltd = LdaModel(corpus=tfidfLtd, id2word=dictionaryLtd, num_topics=10)\n",
    "\n",
    "# tfidfFiltered\n",
    "lda_Filtered = LdaModel(corpus=tfidfFiltered, id2word=dictionaryFiltered, num_topics=10)\n",
    "\n",
    "# tfidfNouns\n",
    "lda_Nouns = LdaModel(corpus=tfidfNouns, id2word=dictionaryNouns, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top keywords in each topic\n",
    "keywords = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    for index, score in sorted(lda_Ltd[tfidfLtd[i]], key=lambda tup: -1*tup[1]):\n",
    "        elements = lda_Ltd.print_topic(index, 5).split(\"+\")\n",
    "        keywords.append([x.strip().replace('\"', '').split(\"*\")[1] for x in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic No: 1\t\n",
      "Score: 0.9272080659866333\t\n",
      "Topic Model: 0.001*\"sayeed\" + 0.001*\"mr\" + 0.001*\"howard\" + 0.001*\"worldcom\" + 0.001*\"pension\"\n"
     ]
    }
   ],
   "source": [
    "# testing on a select output using LDA TF-IDF model\n",
    "x = 51\n",
    "for index, score in sorted(lda_Ltd[tfidfLtd[x]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nTopic No: {}\\t\\nScore: {}\\t\\nTopic Model: {}\".format(index, score, lda_Ltd.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interacting with LDA output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = gensimvis.prepare(lda_model_tfidf, bow_corpus, dic_bow)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating LDA models: Topic coherence\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "goodLdaModel = LdaModel(corpus=bow_corpus, id2word=dic_bow, iterations=50, num_topics=2)\n",
    "badLdaModel = LdaModel(corpus=bow_corpus, id2word=dic_bow, iterations=1, num_topics=2)\n",
    "\n",
    "goodcm = CoherenceModel(model=goodLdaModel, corpus=bow_corpus, dictionary=dic_bow, coherence='u_mass')\n",
    "badcm  = CoherenceModel(model=badLdaModel, corpus=bow_corpus, dictionary=dic_bow, coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodcm = CoherenceModel(model=goodLdaModel, texts=bow_corpus, dictionary=dic_bow, coherence='c_v')\n",
    "badcm  = CoherenceModel(model=badLdaModel, texts=bow_corpus, dictionary=dic_bow, coherence='c_v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel, CoherenceModel\n",
    "\n",
    "# tfidfLtd\n",
    "lsi_Ltd = LsiModel(corpus=tfidfLtd, id2word=dictionaryLtd, num_topics=10)\n",
    "\n",
    "# tfidfFiltered\n",
    "lsi_Filtered = LsiModel(corpus=tfidfFiltered, id2word=dictionaryFiltered, num_topics=10)\n",
    "\n",
    "# tfidfNouns\n",
    "lsi_Nouns = LsiModel(corpus=tfidfNouns, id2word=dictionaryNouns, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Keywords to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfe9ce709e982859ebd8c1b094ee35d9f73a27801040ad55cc46450c9d5cadda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
